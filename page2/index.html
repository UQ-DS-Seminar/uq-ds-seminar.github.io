<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>UQ Data Science Seminar Series | UQ-DS-Seminar</title>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Page 2 of 6 for UQ Data Science Seminar Series | UQ-DS-Seminar</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="UQ Data Science Seminar Series" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="UQ Data Science Seminar Series." />
<meta property="og:description" content="UQ Data Science Seminar Series." />
<meta property="og:site_name" content="UQ-DS-Seminar" />
<meta property="og:type" content="website" />
<link rel="prev" href="/" />
<link rel="next" href="/page3" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="UQ Data Science Seminar Series" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"UQ Data Science Seminar Series.","headline":"UQ Data Science Seminar Series","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/images/logo.png"}},"url":"/page2/"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/uqds.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 


</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>UQ-DS-Seminar</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
    <a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
    <a class="nav-link" href="/calendar">Calendar</a>
</li>
<li class="nav-item">
    <a target="_blank" class="nav-link" href="/categories">By Institutes</a>
</li>
<li class="nav-item">
    <a target="_blank" class="nav-link" href="/tags">By Topics</a>
</li>
<li class="nav-item">
    <a class="nav-link" href="/contact.html">Contact</a>
</li>
            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "/about.html",
    "title": "About",
    "body": "Made with Data Science Discipline, The University of Queensland. "
    }, {
    "id": 2,
    "url": "/author-jane.html",
    "title": "Jane",
    "body": "                        Jane Follow:                                                    Posts by Jane:               "
    }, {
    "id": 3,
    "url": "/author-sal.html",
    "title": "Sal",
    "body": "                        Sal Follow:         https://luoyadan. github. io/                                           Posts by Sal:               "
    }, {
    "id": 4,
    "url": "/authors-list.html",
    "title": "Authors",
    "body": "Authors:                                             Yadan Luo :       (View Posts)                                &nbsp;       &nbsp;                                      "
    }, {
    "id": 5,
    "url": "/buy-me-a-coffee.html",
    "title": "Buy me a coffee",
    "body": "Hi! I am Sal, web designer &amp; developer at WowThemes. net. The free items I create are my side projects and Mundana for Jekyll is one of them. You can find all the work I release for free here. You have my permission to use the free items I develop in your personal, commercial or client projects. If you’d like to reward my work, I would be honored and I could dedicate more time maintaining the free projects. Thank you so much! Buy me a coffee "
    }, {
    "id": 6,
    "url": "/calendar-data/",
    "title": "",
    "body": "[ {	 title : Beyond Visual Geometry- Toward Physical 3D Reconstruction ,	 start :  2025-09-26 ,	 allDay :true,	 url : /chuanxiazheng/ },{	 title : AI-Driven Solutions in Individualised Medicine- From Multimodal Omics Data to Disease Diagnosis and Biomarker Discovery ,	 start :  2025-07-18 ,	 allDay :true,	 url : /hamid/ },{	 title : Teach AI What It Doesn't Know ,	 start :  2025-06-26 ,	 allDay :true,	 url : /xuefeng_du/ },{	 title : Reasoning with Language Models ,	 start :  2025-05-29 ,	 allDay :true,	 url : /llm-nick/ },{	 title : Towards Efficient Novel View Synthesis ,	 start :  2025-05-23 ,	 allDay :true,	 url : /cv-qianyiwu/ },{	 title : Towards Large Generative Recommendation Models: A Tokenization Perspective ,	 start :  2025-05-22 ,	 allDay :true,	 url : /ds-yupenghou/ },{	 title : Would prompt work for graph learning? An exploration of few-shot learning on graphs ,	 start :  2025-04-17 ,	 allDay :true,	 url : /gnn-fangyuan/ },{	 title : Towards Knowledgeable Foundation Models ,	 start :  2025-04-10 ,	 allDay :true,	 url : /knowllm-heng/ },{	 title : AppAgent X—Making GUI Agents Smarter with Use ,	 start :  2025-04-02 ,	 allDay :true,	 url : /cv-chizhang/ },{	 title : Using Large Language Models for Cross-Language Information Access ,	 start :  2025-03-28 ,	 allDay :true,	 url : /nlp-Douglasu/ },{	 title : Leveraging semantics for recommendation at scale ,	 start :  2025-03-26 ,	 allDay :true,	 url : /nlp-Julien/ },{	 title : Towards Multimodal Intelligence: Bridging Vision, Language, and Large-Scale Models ,	 start :  2025-03-07 ,	 allDay :true,	 url : /nlp-jiuxianggu/ },{	 title : From Next Token Prediction to Compliant AI Assistants: A Systematic Path toward Trustworthy Large Language Models ,	 start :  2025-02-28 ,	 allDay :true,	 url : /nlp-yiweiwang/ },{	 title : No. 25-01 Show-o: One Single Transformer to Unify Multimodal Understanding and Generation ,	 start :  2025-02-27 ,	 allDay :true,	 url : /showo-mikeshou/ },{	 title : No. 24-20 Controllable Visual Synthesis via Structural Representation ,	 start :  2024-12-13 ,	 allDay :true,	 url : /synt-yunzhi/ },{	 title : No. 24-19 When LLMs Meet Recommendations: Scalable Hybrid Approaches to Enhance User Experiences ,	 start :  2024-12-09 ,	 allDay :true,	 url : /llmrec-jianling/ },{	 title : No. 24-18 Developing Effective Long-Context Language Models ,	 start :  2024-12-04 ,	 allDay :true,	 url : /ds-lclm-tianyu/ },{	 title : No. 24-17 Mitigating Distribution Shifts in Using Pre-trained Vision-Language Models ,	 start :  2024-12-02 ,	 allDay :true,	 url : /ds-vlm-feng/ },{	 title : No. 24-16 Towards Graph Machine Learning in the Wild ,	 start :  2024-11-27 ,	 allDay :true,	 url : /gnn-wild-qitian/ },{	 title : No. 24-15 Evaluation and Reasoning in Real-world Scenarios ,	 start :  2024-11-20 ,	 allDay :true,	 url : /eval-wenting/ },{	 title : No. 24-14 Long-Range Meets Scalability: Unveiling a Linear-Time Graph Neural Network for Recommendation at Scale ,	 start :  2024-11-13 ,	 allDay :true,	 url : /longrangerecsys-jiahao/ },{	 title : No. 24-13 Contextual Document Embeddings ,	 start :  2024-11-06 ,	 allDay :true,	 url : /context-jack/ },{	 title : No. 24-12 Graph Neural Networks in Epidemic Modeling: An In-Depth Review and Toolkit ,	 start :  2024-10-30 ,	 allDay :true,	 url : /epi-wei/ },{	 title : No. 24-10 Recreating the Physical Natural World from Images ,	 start :  2024-10-09 ,	 allDay :true,	 url : /3d-elliott/ },{	 title : No. 24-09 Human-Computer Conversational Vision-and-Language Navigation ,	 start :  2024-10-08 ,	 allDay :true,	 url : /vlnllm-qi/ },{	 title : No. 24-11 Graph Foundation Model in the Era of LLMs ,	 start :  2024-10-16 ,	 allDay :true,	 url : /llm4graph-chao/ },{	 title : No. 24-08 Towards Graph Foundation Model ,	 start :  2024-09-11 ,	 allDay :true,	 url : /gfm-haitao/ },{	 title : No. 24-07 Embracing Changes in Deep Learning: Continual Learning with Augmented and Modularized Memory ,	 start :  2024-06-11 ,	 allDay :true,	 url : /CL-dong/ },{	 title : No. 24-05 Efficient and Elastic Large Models ,	 start :  2024-05-17 ,	 allDay :true,	 url : /LMs-jain/ },{	 title : No. 24-06 Generative Sequential Recommendation ,	 start :  2024-05-22 ,	 allDay :true,	 url : /genrecsys-sasha/ },{	 title : No. 24-04 Deep Copula-Based Survival Analysis for Dependent Censoring ,	 start :  2024-04-24 ,	 allDay :true,	 url : /survival-weijia/ },{	 title : No. 24-03 Towards Open-World Object Segmentation and Detection ,	 start :  2024-04-10 ,	 allDay :true,	 url : /openworld-jianfei/ },{	 title : No. 24-02 Building AI/ML &amp; Gen AI responsibly on AWS ,	 start :  2024-03-13 ,	 allDay :true,	 url : /amazon-garcia/ },{	 title : No. 24-01 Filter Bubble in Recommender System: Diversity and Beyond ,	 start :  2024-03-07 ,	 allDay :true,	 url : /chen-gao/ },{	 title : Welcome to UQ Data Science Seminar Series! ,	 start :  2023-01-01 ,	 allDay :true,	 url : /welcome-to-uqds/ },{	 title : No. 23-22 Process Mining: Opportunities and Challenges ,	 start :  2023-09-13 ,	 allDay :true,	 url : /process-mining/ },{	 title : No. 23-20 Immersive Data Visualisation and Interactive AI ,	 start :  2023-09-13 ,	 allDay :true,	 url : /immersive-data-visualisation/ },{	 title : No. 23-19 Graph Neural Networks for Large Dynamic Graphs ,	 start :  2023-09-13 ,	 allDay :true,	 url : /dynamic-graph/ },{	 title : No. 23-16 Generalized Out-of-distribution Detection: Theory and Algorithm ,	 start :  2023-08-23 ,	 allDay :true,	 url : /generalized-ood/ },{	 title : No. 23-15 How to Detect Out-of-Distribution Data in the Wild? Challenges, Research Progress and Path Forward ,	 start :  2023-08-16 ,	 allDay :true,	 url : /how-to-detect-ood/ },{	 title : No. 23-13 Task-aware Retrieval with Instructions  ,	 start :  2023-08-02 ,	 allDay :true,	 url : /task-aware-retrieval/ },{	 title : No. 23-12 An Introduction to Sequential/Session-based Recommendation  ,	 start :  2023-05-17 ,	 allDay :true,	 url : /session-recommendation/ },{	 title : No. 23-11 Building Experiment Tracking at Scale with Weights &amp; Biases ,	 start :  2023-05-10 ,	 allDay :true,	 url : /weights-biases/ },{	 title : No. 23-10 SEINE: SEgment-based Indexing for NEural Information Retrieval ,	 start :  2023-05-03 ,	 allDay :true,	 url : /SEINE-retrieval/ },{	 title : No. 23-09 Mobility Digital Twin for Connected and Automated Vehicles ,	 start :  2023-04-26 ,	 allDay :true,	 url : /mobility-digital-twin/ },{	 title : No. 23-05 A magic ingredient, a secret spice, a special blend, for it can all be nice!' The Human Quotient for Better AI Systems ,	 start :  2023-03-22 ,	 allDay :true,	 url : /human-quotient-ai-systems/ },{	 title : No. 23-04 Multi-Domain Few-Shot Image Classification  ,	 start :  2023-03-15 ,	 allDay :true,	 url : /multidomain-fewshot/ },{	 title : No. 23-0301 Efficient Distributed Complex Event Processing ,	 start :  2023-03-09 ,	 allDay :true,	 url : /efficient-event-processing/ },{	 title : No. 23-03 Score based Diffusion Models and Their Applications  ,	 start :  2023-03-01 ,	 allDay :true,	 url : /score-based-diffusiony/ },{	 title : No. 23-02 Seven Algorithms for the Same Task (Testing Uniformity) ,	 start :  2023-03-08 ,	 allDay :true,	 url : /testing-uniformity/ },{	 title : No. 23-01 Escaping the Echo Chamber: The Quest for Normative News Recommender Systems ,	 start :  2023-02-22 ,	 allDay :true,	 url : /escaping-echo/ },{	 title : A Non-Factoid Question-Answering Taxonomy ,	 start :  2022-11-09 ,	 allDay :true,	 url : /QA-taxonomy/ },{	 title : LibAUC: A deep learning library for X-risk Optimization ,	 start :  2022-10-26 ,	 allDay :true,	 url : /libAUC/ },{	 title : Advancing Machine Perception for Artificial Intelligence Systems ,	 start :  2022-10-05 ,	 allDay :true,	 url : /advanced-machine-perception/ }] "
    }, {
    "id": 7,
    "url": "/calendar/",
    "title": "Event Calendar",
    "body": ""
    }, {
    "id": 8,
    "url": "/categories.html",
    "title": "Categories",
    "body": "          Categories               University of Technology Sydney:                                  		No. 23-16 Generalized Out-of-distribution Detection: Theory and Algorithm	: 		  Out-of-distribution (OOD) detection is vital for ensuring the safety and reliability of artificial intelligence systems. It represents a novel and trending area in machine learning an. . . 	 			From 				University of Technology Sydney, 								Aug 23, 2023						                                 		No. 23-12 An Introduction to Sequential/Session-based Recommendation 	: 		  In recent years, sequential/session-based recommendations have emerged as a new recommendation paradigm to well model users’ dynamic and short-term preferences for more accurate and t. . . 	 			From 				University of Technology Sydney, 								May 17, 2023						                                 		Advancing Machine Perception for Artificial Intelligence Systems	: 		  Artificial intelligence (AI) techniques have impacted on our lives profoundly, such as providing more secured and resilient social environments and assisting more accurate medical dia. . . 	 			From 				University of Technology Sydney, 								Oct 05, 2022						                              Texas A&amp;M University:                                  		LibAUC: A deep learning library for X-risk Optimization	: 		  In this talk, I will present our recent research efforts of developing a deep learning library called LibAUC, which is applicable for solving a variety of compositional measures.  I w. . . 	 			From 				Texas A&amp;M University, 								Oct 26, 2022						                              RMIT University:                                  		A Non-Factoid Question-Answering Taxonomy	: 		  Non-factoid question answering (NFQA) is a challenging and under-researched task that requires constructing long-form answers, such as explanations or opinions, to open-ended non-fact. . . 	 			From 				RMIT University, 								Nov 09, 2022						                              University of Zurich:                                  		No. 23-01 Escaping the Echo Chamber: The Quest for Normative News Recommender Systems	: 		  Recommender systems and social networks are often faulted to be the cause for creating Echo Chambers – environments where people mostly encounter news that match their previous choice. . . 	 			From 				University of Zurich, 								Feb 22, 2023						                              University of Sydney:                                  		No. 23-02 Seven Algorithms for the Same Task (Testing Uniformity)	: 		  Suppose you get a set of (independent) data points in some discrete but huge domain {1,2,…,k}, and want to determine if this data is uniformly distributed. This is a basic and fundame. . . 	 			From 				University of Sydney, 								Mar 08, 2023						                              Australian National University:                                  		No. 23-04 Multi-Domain Few-Shot Image Classification 	: 		  Most existing few-shot classification methods only consider generalization on one dataset (i. e. , single-domain), failing to transfer across various seen and unseen domains. In this ta. . . 	 			From 				Australian National University, 								Mar 15, 2023						                                 		No. 23-03 Score based Diffusion Models and Their Applications 	: 		  Generative models show great potential in generating new samples, which have been extensively investigated in 2D/3D vision tasks. Among them, the adversarial training based models, e. . . . 	 			From 				Australian National University, 								Mar 01, 2023						                              Humboldt-Universität zu Berlin (HU):                                  		No. 23-0301 Efficient Distributed Complex Event Processing	: 		  Complex event processing emerged as a computational paradigm to detect patterns in event streams based on the continuous evaluation of event queries. Once such queries are evaluated i. . . 	 			From 				Humboldt-Universität zu Berlin (HU), 								Mar 09, 2023						                              Delft University of Technology:                                  		No. 23-05 A magic ingredient, a secret spice, a special blend, for it can all be nice!' The Human Quotient for Better AI Systems	: 		  The unprecedented rise in the adoption of artificial intelligence techniques and automation in many contexts is concomitant with the shortcomings of such technology concerning robustn. . . 	 			From 				Delft University of Technology, 								Mar 22, 2023						                              Purdue University:                                  		No. 23-09 Mobility Digital Twin for Connected and Automated Vehicles	: 		  A Digital Twin is a digital replica of a living or nonliving physical entity, and this emerging technology attracted extensive attention from different industries during the past deca. . . 	 			From 				Purdue University, 								Apr 26, 2023						                              Georgetown University:                                  		No. 23-10 SEINE: SEgment-based Indexing for NEural Information Retrieval	: 		  Many early neural Information Retrieval (NeurIR) methods are re-rankers that rely on a traditional first-stage retriever due to expensive query time computations. Recently, representa. . . 	 			From 				Georgetown University, 								May 03, 2023						                              Weights &amp; Biases:                                  		No. 23-11 Building Experiment Tracking at Scale with Weights &amp; Biases	: 		  Building experiments doesn’t just end once the model is deployed. Teams need to monitor their models in production and use their findings to iterate further. Especially when dealing w. . . 	 			From 				Weights &amp; Biases, 								May 10, 2023						                              University of Washington:                                  		No. 23-13 Task-aware Retrieval with Instructions 	: 		  We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose t. . . 	 			From 				University of Washington, 								Aug 02, 2023						                              University of Wisconsin-Madison:                                  		No. 23-15 How to Detect Out-of-Distribution Data in the Wild? Challenges, Research Progress and Path Forward	: 		  When deploying machine learning models in the open and non-stationary world, their reliability is often challenged by the presence of out-of-distribution (OOD) samples. Since data shi. . . 	 			From 				University of Wisconsin-Madison, 								Aug 16, 2023						                              DATA61-CSIRO:                                  		No. 23-19 Graph Neural Networks for Large Dynamic Graphs	: 		  In real-world applications such as social networks, financial transactions, and recommender systems, graph-structured data is frequently dynamic, with the nodes and edges of the graph. . . 	 			From 				DATA61-CSIRO, 								Sep 13, 2023						                              University of Queensland:                                  		No. 23-22 Process Mining: Opportunities and Challenges	: 		  In recent years, significant advances in extended reality (XR) and AI have offered novel, promising ways to visualise and eventually better understand large and complex data. Immersio. . . 	 			From 				University of Queensland, 								Sep 13, 2023						                                 		No. 23-20 Immersive Data Visualisation and Interactive AI	: 		  In recent years, significant advances in extended reality (XR) and AI have offered novel, promising ways to visualise and eventually better understand large and complex data. Immersio. . . 	 			From 				University of Queensland, 								Sep 13, 2023						                              Introduction:                                  		Welcome to UQ Data Science Seminar Series!	: 		  This UQ Data Science seminar series aims to bring together students and senior researchers to discuss about their research, intending to have a diverse set of talks and speakers on to. . . 	 			From 				Introduction, 								Jan 01, 2023						                              Tsinghua University:                                  		No. 24-01 Filter Bubble in Recommender System: Diversity and Beyond	: 		  Recommender system has reshaped how we access information in today’s world, make relevant content more accessible to everyone. However, it has also resulted in some negative side-effe. . . 	 			From 				Tsinghua University, 								Mar 07, 2024						                              Amazon:                                  		Leveraging semantics for recommendation at scale	: 		  In this talk, we present some of our recent work conducted at Amazon International Machine Learning Australia. First, we present a simple approach to address cold-start recommendation. . . 	 			From 				Amazon, 								Mar 26, 2025						                                 		No. 24-02 Building AI/ML &amp; Gen AI responsibly on AWS	: 		  This presentation explores best practices for building AI/ML and generative AI (Gen AI) models responsibly on AWS. We’ll explore real use cases where these technologies are driving va. . . 	 			From 				Amazon, 								Mar 13, 2024						                              Monash University:                                  		Towards Efficient Novel View Synthesis	: 		  Novel view synthesis (NVS) from 2D images aims to generate unseen views of a scene given multiple input observations. It is a fundamental task in computer vision that has garnered sig. . . 	 			From 				Monash University, 								May 23, 2025						                                 		No. 24-03 Towards Open-World Object Segmentation and Detection	: 		  Segmentation and detection are two fundamental and classical tasks in computer vision. In recent years, significant attention has been devoted to the open-vocabulary object segmentati. . . 	 			From 				Monash University, 								Apr 10, 2024						                              University of Newcastle:                                  		No. 24-04 Deep Copula-Based Survival Analysis for Dependent Censoring	: 		  Censoring is the central problem in survival analysis where either the time-to-event (for instance, death) or the time-to-censoring (such as loss of follow-up) is observed for each sa. . . 	 			From 				University of Newcastle, 								Apr 24, 2024						                              University of Glasgow:                                  		No. 24-06 Generative Sequential Recommendation	: 		  In this talk, we first introduce the Sequential Recommendation problem and draw parallels between language modelling and recommender systems. To set the stage, we also briefly cover s. . . 	 			From 				University of Glasgow, 								May 22, 2024						                              Google Research India:                                  		No. 24-05 Efficient and Elastic Large Models	: 		  Generative LLMs are transforming multiple industries and have proven to be robust for multitude of use cases across industries and settings. One of the key impediments to their widesp. . . 	 			From 				Google Research India, 								May 17, 2024						                              The University of New South Wales:                                  		No. 24-07 Embracing Changes in Deep Learning: Continual Learning with Augmented and Modularized Memory	: 		  Deep learning (DL) has been successful in many applications. However, the conventional DL approaches focus on the end results on fixed datasets/scenarios and fail to handle the dynami. . . 	 			From 				The University of New South Wales, 								Jun 11, 2024						                              Michigan State University:                                  		No. 24-08 Towards Graph Foundation Model	: 		  Graph Foundation Models (GFMs) is a single (neural) model that learns transferable graph representations that can generalize to any new, previously unseen graph. In this talk, we will. . . 	 			From 				Michigan State University, 								Sep 11, 2024						                              University of Hong Kong:                                  		No. 24-11 Graph Foundation Model in the Era of LLMs	: 		  Graph data structures play a crucial role in real life, effectively illustrating the complex relationships and structural dependencies between entities. In recent years, the generaliz. . . 	 			From 				University of Hong Kong, 								Oct 16, 2024						                              University of Adelaide:                                  		No. 24-09 Human-Computer Conversational Vision-and-Language Navigation	: 		  The dynamic realm of Vision-and-Language Navigation (VLN) has garnered significant multidisciplinary interest, resonating within the domains of computer vision, natural language proce. . . 	 			From 				University of Adelaide, 								Oct 08, 2024						                              Stanford University:                                  		No. 24-10 Recreating the Physical Natural World from Images	: 		  Today, generative AI models excel at creating visual worlds through pixels, but still often struggle with the comprehension of basic physical concepts such as 3D shape, motion, materi. . . 	 			From 				Stanford University, 								Oct 09, 2024						                              Emory University:                                  		No. 24-12 Graph Neural Networks in Epidemic Modeling: An In-Depth Review and Toolkit	: 		  Since the onset of the COVID-19 pandemic, there has been growing interest in epidemic modeling. While traditional mechanistic models effectively describe the mathematical dynamics of . . . 	 			From 				Emory University, 								Oct 30, 2024						                              Cornell University:                                  		No. 24-15 Evaluation and Reasoning in Real-world Scenarios	: 		  User queries in natural settings, such as “provide a design for a disk topology for a NAS built on TrueNAS Scale, as well as a dataset layout,” differ significantly from those produce. . . 	 			From 				Cornell University, 								Nov 20, 2024						                                 		No. 24-13 Contextual Document Embeddings	: 		  Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this wor. . . 	 			From 				Cornell University, 								Nov 06, 2024						                              Pennsylvania State University:                                  		No. 24-14 Long-Range Meets Scalability: Unveiling a Linear-Time Graph Neural Network for Recommendation at Scale	: 		  Recommender systems play a central role in shaping our daily digital experiences, yet achieving both scalability and expressive power remains a significant challenge. While Graph Neur. . . 	 			From 				Pennsylvania State University, 								Nov 13, 2024						                              MIT:                                  		No. 24-16 Towards Graph Machine Learning in the Wild	: 		  Learning on graphs is a long-standing and fundamental challenge in machine learning and recent works have demonstrated solid progress in this area. However, most existing models tacit. . . 	 			From 				MIT, 								Nov 27, 2024						                              UniMelb:                                  		No. 24-17 Mitigating Distribution Shifts in Using Pre-trained Vision-Language Models	: 		  Benefiting from large-scale image-text pair datasets, powerful pre-trained vision-language models (VLMs, such as CLIP) enable many real-world applications, e. g. , zero-shot classificat. . . 	 			From 				UniMelb, 								Dec 02, 2024						                              Princeton:                                  		No. 24-18 Developing Effective Long-Context Language Models	: 		  In this talk, I will share our journey behind developing an effective long-context language model. I’ll begin by introducing our initial approach of using parallel context encoding (C. . . 	 			From 				Princeton, 								Dec 04, 2024						                              Deepmind:                                  		No. 24-19 When LLMs Meet Recommendations: Scalable Hybrid Approaches to Enhance User Experiences	: 		  While LLMs offer powerful reasoning and generalization capabilities for user understanding and long-term planning in recommendation systems, their latency and cost hinder direct appli. . . 	 			From 				Deepmind, 								Dec 09, 2024						                              Stanford:                                  		No. 24-20 Controllable Visual Synthesis via Structural Representation	: 		  End-to-end neural approaches have revolutionized visual generation, producing stunning outputs from natural language prompts. However, precise controls remain challenging through dire. . . 	 			From 				Stanford, 								Dec 13, 2024						                              NUS:                                  		No. 25-01 Show-o: One Single Transformer to Unify Multimodal Understanding and Generation	: 		  Exciting models have been developed in multimodal video understanding and generation, such as video LLM and video diffusion model. One emerging pathway to the ultimate intelligence is. . . 	 			From 				NUS, 								Feb 27, 2025						                              UC Merced:                                  		From Next Token Prediction to Compliant AI Assistants: A Systematic Path toward Trustworthy Large Language Models	: 		  Language models are systems that can predict upcoming words” - this classical definition of NLP models forms the basis of LLMs becoming responsive text completion models. However, suc. . . 	 			From 				UC Merced, 								Feb 28, 2025						                              Adobe:                                  		Towards Multimodal Intelligence: Bridging Vision, Language, and Large-Scale Models	: 		  Multimodal intelligence is revolutionizing document understanding by enabling AI to process and reason across vision and language. This talk explores how large-scale models integrate . . . 	 			From 				Adobe, 								Mar 07, 2025						                              University of Maryland:                                  		Using Large Language Models for Cross-Language Information Access	: 		  One interesting aspect of today’s generative Large Language Models (LLMs) is that they are natural polyglots, facile in many languages.    These new multi-dexterous capabilities offer . . . 	 			From 				University of Maryland, 								Mar 28, 2025						                              Westlake University:                                  		AppAgent X—Making GUI Agents Smarter with Use	: 		  In recent years, the development of multimodal large language models has given rise to a new class of intelligent agents—GUI Agents—that can autonomously operate computers and smartph. . . 	 			From 				Westlake University, 								Apr 02, 2025						                              University of Illinois Urbana-Champaign:                                  		Towards Knowledgeable Foundation Models	: 		  Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance on knowledge reasoning tasks, owing to their implicit knowledge derived from ex. . . 	 			From 				University of Illinois Urbana-Champaign, 								Apr 10, 2025						                              Singapore Management University (SMU):                                  		Would prompt work for graph learning? An exploration of few-shot learning on graphs	: 		  Graph structures are prevalent across a variety of fields, including social networks, e-commerce, transportation, and biological systems. Within these graphs, numerous analytical and . . . 	 			From 				Singapore Management University (SMU), 								Apr 17, 2025						                              University of California:                                  		Towards Large Generative Recommendation Models: A Tokenization Perspective	: 		  The emergence of large generative models is transforming the landscape of recommender systems, offering new opportunities through scaling laws and flexible content generation. One of . . . 	 			From 				University of California, 				San Diego (UCSD), 								May 22, 2025						                              San Diego (UCSD):                                  		Towards Large Generative Recommendation Models: A Tokenization Perspective	: 		  The emergence of large generative models is transforming the landscape of recommender systems, offering new opportunities through scaling laws and flexible content generation. One of . . . 	 			From 				University of California, 				San Diego (UCSD), 								May 22, 2025						                              UC Berkeley:                                  		Reasoning with Language Models	: 		  Language models are primarily trained via imitation on massive amounts of human data; as a result, they’re capable of performing a wide range of tasks but often lack the deep reasonin. . . 	 			From 				UC Berkeley, 								May 29, 2025						                              NTU:                                  		Beyond Visual Geometry- Toward Physical 3D Reconstruction	: 		  Recent progress in 3D Visual Geometry has led to impressive reconstruction and generation results using purely feed-forward deep networks. However, much of this progress remains limit. . . 	 			From 				NTU, 								Sep 26, 2025						                                 		Teach AI What It Doesn't Know	: 		  The remarkable capabilities of machine learning (ML) models, especially foundation models like GPT, have transformed numerous domains. However, these systems often falter in real-worl. . . 	 			From 				NTU, 								Jun 26, 2025						                              UNSW:                                  		AI-Driven Solutions in Individualised Medicine- From Multimodal Omics Data to Disease Diagnosis and Biomarker Discovery	: 		  Despite significant advancements in medical science and an increasing emphasis on precision medicine, the majority of medical diagnoses are still made after patients exhibit noticeabl. . . 	 			From 				UNSW, 								Jul 18, 2025						                                             Upcoming:    				                  "
    }, {
    "id": 9,
    "url": "/contact.html",
    "title": "Contact",
    "body": "  Please send your message to UQ-DS-Seminar. We will reply as soon as possible!   "
    }, {
    "id": 10,
    "url": "/",
    "title": "UQ Data Science Seminar Series",
    "body": "                                Beyond Visual Geometry- Toward Physical 3D Reconstruction  :       Recent progress in 3D Visual Geometry has led to impressive reconstruction and generation results using purely feed-forward deep netw. . .               From                 NTU,                                        Sep 26, 2025                                                                                                                             AI-Driven Solutions in Individualised Medicine- From Multimodal Omics Data to Disease Diagnosis and Biomarker Discovery          :                       From                         UNSW,                                                                  Jul 18, 2025                                                                                                                                     Teach AI What It Doesn't Know          :                       From                         NTU,                                                                  Jun 26, 2025                                                                                                                                    Reasoning with Language Models          :                       From                         UC Berkeley,                                                                  May 29, 2025                                                                             All Talks:                   		Beyond Visual Geometry- Toward Physical 3D Reconstruction	: 		  Recent progress in 3D Visual Geometry has led to impressive reconstruction and generation results using purely feed-forward deep networks. However, much of this progress remains limit. . . 	 			From 				NTU, 								Sep 26, 2025						                  		AI-Driven Solutions in Individualised Medicine- From Multimodal Omics Data to Disease Diagnosis and Biomarker Discovery	: 		  Despite significant advancements in medical science and an increasing emphasis on precision medicine, the majority of medical diagnoses are still made after patients exhibit noticeabl. . . 	 			From 				UNSW, 								Jul 18, 2025						                  		Teach AI What It Doesn't Know	: 		  The remarkable capabilities of machine learning (ML) models, especially foundation models like GPT, have transformed numerous domains. However, these systems often falter in real-worl. . . 	 			From 				NTU, 								Jun 26, 2025						                  		Reasoning with Language Models	: 		  Language models are primarily trained via imitation on massive amounts of human data; as a result, they’re capable of performing a wide range of tasks but often lack the deep reasonin. . . 	 			From 				UC Berkeley, 								May 29, 2025						                  		Towards Efficient Novel View Synthesis	: 		  Novel view synthesis (NVS) from 2D images aims to generate unseen views of a scene given multiple input observations. It is a fundamental task in computer vision that has garnered sig. . . 	 			From 				Monash University, 								May 23, 2025						                  		Towards Large Generative Recommendation Models: A Tokenization Perspective	: 		  The emergence of large generative models is transforming the landscape of recommender systems, offering new opportunities through scaling laws and flexible content generation. One of . . . 	 			From 				University of California, 				San Diego (UCSD), 								May 22, 2025						                  		Would prompt work for graph learning? An exploration of few-shot learning on graphs	: 		  Graph structures are prevalent across a variety of fields, including social networks, e-commerce, transportation, and biological systems. Within these graphs, numerous analytical and . . . 	 			From 				Singapore Management University (SMU), 								Apr 17, 2025						                  		Towards Knowledgeable Foundation Models	: 		  Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance on knowledge reasoning tasks, owing to their implicit knowledge derived from ex. . . 	 			From 				University of Illinois Urbana-Champaign, 								Apr 10, 2025						                  		AppAgent X—Making GUI Agents Smarter with Use	: 		  In recent years, the development of multimodal large language models has given rise to a new class of intelligent agents—GUI Agents—that can autonomously operate computers and smartph. . . 	 			From 				Westlake University, 								Apr 02, 2025						                  		Using Large Language Models for Cross-Language Information Access	: 		  One interesting aspect of today’s generative Large Language Models (LLMs) is that they are natural polyglots, facile in many languages.    These new multi-dexterous capabilities offer . . . 	 			From 				University of Maryland, 								Mar 28, 2025						                                                &laquo;                              1                               2                               3                               4                               5                               6                              Next &raquo;                                          Upcoming:    				              "
    }, {
    "id": 11,
    "url": "/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "“UQ-DS-Seminar” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 12,
    "url": "/tags.html",
    "title": "Tags",
    "body": "          Tags               Computer Vision:                                  		No. 24-03 Towards Open-World Object Segmentation and Detection	: 		  Segmentation and detection are two fundamental and classical tasks in computer vision. In recent years, significant attention has been devoted to the open-vocabulary object segmentati. . . 	 			From 				Monash University, 								Apr 10, 2024						                                 		No. 23-04 Multi-Domain Few-Shot Image Classification 	: 		  Most existing few-shot classification methods only consider generalization on one dataset (i. e. , single-domain), failing to transfer across various seen and unseen domains. In this ta. . . 	 			From 				Australian National University, 								Mar 15, 2023						                                 		No. 23-03 Score based Diffusion Models and Their Applications 	: 		  Generative models show great potential in generating new samples, which have been extensively investigated in 2D/3D vision tasks. Among them, the adversarial training based models, e. . . . 	 			From 				Australian National University, 								Mar 01, 2023						                                 		Advancing Machine Perception for Artificial Intelligence Systems	: 		  Artificial intelligence (AI) techniques have impacted on our lives profoundly, such as providing more secured and resilient social environments and assisting more accurate medical dia. . . 	 			From 				University of Technology Sydney, 								Oct 05, 2022						                              Optimization:                                  		LibAUC: A deep learning library for X-risk Optimization	: 		  In this talk, I will present our recent research efforts of developing a deep learning library called LibAUC, which is applicable for solving a variety of compositional measures.  I w. . . 	 			From 				Texas A&amp;M University, 								Oct 26, 2022						                              Information Retrieval:                                  		No. 23-13 Task-aware Retrieval with Instructions 	: 		  We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose t. . . 	 			From 				University of Washington, 								Aug 02, 2023						                                 		No. 23-10 SEINE: SEgment-based Indexing for NEural Information Retrieval	: 		  Many early neural Information Retrieval (NeurIR) methods are re-rankers that rely on a traditional first-stage retriever due to expensive query time computations. Recently, representa. . . 	 			From 				Georgetown University, 								May 03, 2023						                                 		A Non-Factoid Question-Answering Taxonomy	: 		  Non-factoid question answering (NFQA) is a challenging and under-researched task that requires constructing long-form answers, such as explanations or opinions, to open-ended non-fact. . . 	 			From 				RMIT University, 								Nov 09, 2022						                              Information Systems:                                  		No. 23-22 Process Mining: Opportunities and Challenges	: 		  In recent years, significant advances in extended reality (XR) and AI have offered novel, promising ways to visualise and eventually better understand large and complex data. Immersio. . . 	 			From 				University of Queensland, 								Sep 13, 2023						                                 		No. 23-05 A magic ingredient, a secret spice, a special blend, for it can all be nice!' The Human Quotient for Better AI Systems	: 		  The unprecedented rise in the adoption of artificial intelligence techniques and automation in many contexts is concomitant with the shortcomings of such technology concerning robustn. . . 	 			From 				Delft University of Technology, 								Mar 22, 2023						                                 		No. 23-0301 Efficient Distributed Complex Event Processing	: 		  Complex event processing emerged as a computational paradigm to detect patterns in event streams based on the continuous evaluation of event queries. Once such queries are evaluated i. . . 	 			From 				Humboldt-Universität zu Berlin (HU), 								Mar 09, 2023						                                 		No. 23-01 Escaping the Echo Chamber: The Quest for Normative News Recommender Systems	: 		  Recommender systems and social networks are often faulted to be the cause for creating Echo Chambers – environments where people mostly encounter news that match their previous choice. . . 	 			From 				University of Zurich, 								Feb 22, 2023						                              Theory:                                  		No. 23-02 Seven Algorithms for the Same Task (Testing Uniformity)	: 		  Suppose you get a set of (independent) data points in some discrete but huge domain {1,2,…,k}, and want to determine if this data is uniformly distributed. This is a basic and fundame. . . 	 			From 				University of Sydney, 								Mar 08, 2023						                              Digital Twin:                                  		No. 23-09 Mobility Digital Twin for Connected and Automated Vehicles	: 		  A Digital Twin is a digital replica of a living or nonliving physical entity, and this emerging technology attracted extensive attention from different industries during the past deca. . . 	 			From 				Purdue University, 								Apr 26, 2023						                              Tools:                                  		No. 23-11 Building Experiment Tracking at Scale with Weights &amp; Biases	: 		  Building experiments doesn’t just end once the model is deployed. Teams need to monitor their models in production and use their findings to iterate further. Especially when dealing w. . . 	 			From 				Weights &amp; Biases, 								May 10, 2023						                              Recommendation:                                  		Towards Large Generative Recommendation Models: A Tokenization Perspective	: 		  The emergence of large generative models is transforming the landscape of recommender systems, offering new opportunities through scaling laws and flexible content generation. One of . . . 	 			From 				University of California, 				San Diego (UCSD), 								May 22, 2025						                                 		Leveraging semantics for recommendation at scale	: 		  In this talk, we present some of our recent work conducted at Amazon International Machine Learning Australia. First, we present a simple approach to address cold-start recommendation. . . 	 			From 				Amazon, 								Mar 26, 2025						                                 		No. 24-01 Filter Bubble in Recommender System: Diversity and Beyond	: 		  Recommender system has reshaped how we access information in today’s world, make relevant content more accessible to everyone. However, it has also resulted in some negative side-effe. . . 	 			From 				Tsinghua University, 								Mar 07, 2024						                                 		No. 23-12 An Introduction to Sequential/Session-based Recommendation 	: 		  In recent years, sequential/session-based recommendations have emerged as a new recommendation paradigm to well model users’ dynamic and short-term preferences for more accurate and t. . . 	 			From 				University of Technology Sydney, 								May 17, 2023						                              Machine Learning:                                  		No. 23-19 Graph Neural Networks for Large Dynamic Graphs	: 		  In real-world applications such as social networks, financial transactions, and recommender systems, graph-structured data is frequently dynamic, with the nodes and edges of the graph. . . 	 			From 				DATA61-CSIRO, 								Sep 13, 2023						                                 		No. 23-16 Generalized Out-of-distribution Detection: Theory and Algorithm	: 		  Out-of-distribution (OOD) detection is vital for ensuring the safety and reliability of artificial intelligence systems. It represents a novel and trending area in machine learning an. . . 	 			From 				University of Technology Sydney, 								Aug 23, 2023						                                 		No. 23-15 How to Detect Out-of-Distribution Data in the Wild? Challenges, Research Progress and Path Forward	: 		  When deploying machine learning models in the open and non-stationary world, their reliability is often challenged by the presence of out-of-distribution (OOD) samples. Since data shi. . . 	 			From 				University of Wisconsin-Madison, 								Aug 16, 2023						                              Visualisation:                                  		No. 23-20 Immersive Data Visualisation and Interactive AI	: 		  In recent years, significant advances in extended reality (XR) and AI have offered novel, promising ways to visualise and eventually better understand large and complex data. Immersio. . . 	 			From 				University of Queensland, 								Sep 13, 2023						                              welcome:                                  		Welcome to UQ Data Science Seminar Series!	: 		  This UQ Data Science seminar series aims to bring together students and senior researchers to discuss about their research, intending to have a diverse set of talks and speakers on to. . . 	 			From 				Introduction, 								Jan 01, 2023						                              Industry:                                  		No. 24-02 Building AI/ML &amp; Gen AI responsibly on AWS	: 		  This presentation explores best practices for building AI/ML and generative AI (Gen AI) models responsibly on AWS. We’ll explore real use cases where these technologies are driving va. . . 	 			From 				Amazon, 								Mar 13, 2024						                              Medical:                                  		No. 24-04 Deep Copula-Based Survival Analysis for Dependent Censoring	: 		  Censoring is the central problem in survival analysis where either the time-to-event (for instance, death) or the time-to-censoring (such as loss of follow-up) is observed for each sa. . . 	 			From 				University of Newcastle, 								Apr 24, 2024						                              RecSys:                                  		No. 24-19 When LLMs Meet Recommendations: Scalable Hybrid Approaches to Enhance User Experiences	: 		  While LLMs offer powerful reasoning and generalization capabilities for user understanding and long-term planning in recommendation systems, their latency and cost hinder direct appli. . . 	 			From 				Deepmind, 								Dec 09, 2024						                                 		No. 24-15 Evaluation and Reasoning in Real-world Scenarios	: 		  User queries in natural settings, such as “provide a design for a disk topology for a NAS built on TrueNAS Scale, as well as a dataset layout,” differ significantly from those produce. . . 	 			From 				Cornell University, 								Nov 20, 2024						                                 		No. 24-14 Long-Range Meets Scalability: Unveiling a Linear-Time Graph Neural Network for Recommendation at Scale	: 		  Recommender systems play a central role in shaping our daily digital experiences, yet achieving both scalability and expressive power remains a significant challenge. While Graph Neur. . . 	 			From 				Pennsylvania State University, 								Nov 13, 2024						                                 		No. 24-06 Generative Sequential Recommendation	: 		  In this talk, we first introduce the Sequential Recommendation problem and draw parallels between language modelling and recommender systems. To set the stage, we also briefly cover s. . . 	 			From 				University of Glasgow, 								May 22, 2024						                              LLM:                                  		AI-Driven Solutions in Individualised Medicine- From Multimodal Omics Data to Disease Diagnosis and Biomarker Discovery	: 		  Despite significant advancements in medical science and an increasing emphasis on precision medicine, the majority of medical diagnoses are still made after patients exhibit noticeabl. . . 	 			From 				UNSW, 								Jul 18, 2025						                                 		Would prompt work for graph learning? An exploration of few-shot learning on graphs	: 		  Graph structures are prevalent across a variety of fields, including social networks, e-commerce, transportation, and biological systems. Within these graphs, numerous analytical and . . . 	 			From 				Singapore Management University (SMU), 								Apr 17, 2025						                                 		Towards Knowledgeable Foundation Models	: 		  Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance on knowledge reasoning tasks, owing to their implicit knowledge derived from ex. . . 	 			From 				University of Illinois Urbana-Champaign, 								Apr 10, 2025						                                 		AppAgent X—Making GUI Agents Smarter with Use	: 		  In recent years, the development of multimodal large language models has given rise to a new class of intelligent agents—GUI Agents—that can autonomously operate computers and smartph. . . 	 			From 				Westlake University, 								Apr 02, 2025						                                 		Using Large Language Models for Cross-Language Information Access	: 		  One interesting aspect of today’s generative Large Language Models (LLMs) is that they are natural polyglots, facile in many languages.    These new multi-dexterous capabilities offer . . . 	 			From 				University of Maryland, 								Mar 28, 2025						                                 		From Next Token Prediction to Compliant AI Assistants: A Systematic Path toward Trustworthy Large Language Models	: 		  Language models are systems that can predict upcoming words” - this classical definition of NLP models forms the basis of LLMs becoming responsive text completion models. However, suc. . . 	 			From 				UC Merced, 								Feb 28, 2025						                                 		No. 24-05 Efficient and Elastic Large Models	: 		  Generative LLMs are transforming multiple industries and have proven to be robust for multitude of use cases across industries and settings. One of the key impediments to their widesp. . . 	 			From 				Google Research India, 								May 17, 2024						                              CV:                                  		No. 25-01 Show-o: One Single Transformer to Unify Multimodal Understanding and Generation	: 		  Exciting models have been developed in multimodal video understanding and generation, such as video LLM and video diffusion model. One emerging pathway to the ultimate intelligence is. . . 	 			From 				NUS, 								Feb 27, 2025						                                 		No. 24-07 Embracing Changes in Deep Learning: Continual Learning with Augmented and Modularized Memory	: 		  Deep learning (DL) has been successful in many applications. However, the conventional DL approaches focus on the end results on fixed datasets/scenarios and fail to handle the dynami. . . 	 			From 				The University of New South Wales, 								Jun 11, 2024						                              GNN:                                  		No. 24-16 Towards Graph Machine Learning in the Wild	: 		  Learning on graphs is a long-standing and fundamental challenge in machine learning and recent works have demonstrated solid progress in this area. However, most existing models tacit. . . 	 			From 				MIT, 								Nov 27, 2024						                                 		No. 24-13 Contextual Document Embeddings	: 		  Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this wor. . . 	 			From 				Cornell University, 								Nov 06, 2024						                                 		No. 24-12 Graph Neural Networks in Epidemic Modeling: An In-Depth Review and Toolkit	: 		  Since the onset of the COVID-19 pandemic, there has been growing interest in epidemic modeling. While traditional mechanistic models effectively describe the mathematical dynamics of . . . 	 			From 				Emory University, 								Oct 30, 2024						                                 		No. 24-11 Graph Foundation Model in the Era of LLMs	: 		  Graph data structures play a crucial role in real life, effectively illustrating the complex relationships and structural dependencies between entities. In recent years, the generaliz. . . 	 			From 				University of Hong Kong, 								Oct 16, 2024						                                 		No. 24-08 Towards Graph Foundation Model	: 		  Graph Foundation Models (GFMs) is a single (neural) model that learns transferable graph representations that can generalize to any new, previously unseen graph. In this talk, we will. . . 	 			From 				Michigan State University, 								Sep 11, 2024						                              Visual Language:                                  		No. 24-09 Human-Computer Conversational Vision-and-Language Navigation	: 		  The dynamic realm of Vision-and-Language Navigation (VLN) has garnered significant multidisciplinary interest, resonating within the domains of computer vision, natural language proce. . . 	 			From 				University of Adelaide, 								Oct 08, 2024						                              3D:                                  		Beyond Visual Geometry- Toward Physical 3D Reconstruction	: 		  Recent progress in 3D Visual Geometry has led to impressive reconstruction and generation results using purely feed-forward deep networks. However, much of this progress remains limit. . . 	 			From 				NTU, 								Sep 26, 2025						                                 		No. 24-20 Controllable Visual Synthesis via Structural Representation	: 		  End-to-end neural approaches have revolutionized visual generation, producing stunning outputs from natural language prompts. However, precise controls remain challenging through dire. . . 	 			From 				Stanford, 								Dec 13, 2024						                                 		No. 24-10 Recreating the Physical Natural World from Images	: 		  Today, generative AI models excel at creating visual worlds through pixels, but still often struggle with the comprehension of basic physical concepts such as 3D shape, motion, materi. . . 	 			From 				Stanford University, 								Oct 09, 2024						                              Vision-Language:                                  		No. 24-17 Mitigating Distribution Shifts in Using Pre-trained Vision-Language Models	: 		  Benefiting from large-scale image-text pair datasets, powerful pre-trained vision-language models (VLMs, such as CLIP) enable many real-world applications, e. g. , zero-shot classificat. . . 	 			From 				UniMelb, 								Dec 02, 2024						                              NLP:                                  		No. 24-18 Developing Effective Long-Context Language Models	: 		  In this talk, I will share our journey behind developing an effective long-context language model. I’ll begin by introducing our initial approach of using parallel context encoding (C. . . 	 			From 				Princeton, 								Dec 04, 2024						                              MultiModal:                                  		Towards Multimodal Intelligence: Bridging Vision, Language, and Large-Scale Models	: 		  Multimodal intelligence is revolutionizing document understanding by enabling AI to process and reason across vision and language. This talk explores how large-scale models integrate . . . 	 			From 				Adobe, 								Mar 07, 2025						                              cv:                                  		Towards Efficient Novel View Synthesis	: 		  Novel view synthesis (NVS) from 2D images aims to generate unseen views of a scene given multiple input observations. It is a fundamental task in computer vision that has garnered sig. . . 	 			From 				Monash University, 								May 23, 2025						                              llm:                                  		Reasoning with Language Models	: 		  Language models are primarily trained via imitation on massive amounts of human data; as a result, they’re capable of performing a wide range of tasks but often lack the deep reasonin. . . 	 			From 				UC Berkeley, 								May 29, 2025						                              ML:                                  		Teach AI What It Doesn't Know	: 		  The remarkable capabilities of machine learning (ML) models, especially foundation models like GPT, have transformed numerous domains. However, these systems often falter in real-worl. . . 	 			From 				NTU, 								Jun 26, 2025						                                             Upcoming:    				                  "
    }, {
    "id": 13,
    "url": "/robots.txt",
    "title": "",
    "body": "Sitemap: /sitemap. xml "
    }, {
    "id": 14,
    "url": "/page2/",
    "title": "UQ Data Science Seminar Series",
    "body": "                  All Talks:                   		Leveraging semantics for recommendation at scale	: 		  In this talk, we present some of our recent work conducted at Amazon International Machine Learning Australia. First, we present a simple approach to address cold-start recommendation. . . 	 			From 				Amazon, 								Mar 26, 2025						                  		Towards Multimodal Intelligence: Bridging Vision, Language, and Large-Scale Models	: 		  Multimodal intelligence is revolutionizing document understanding by enabling AI to process and reason across vision and language. This talk explores how large-scale models integrate . . . 	 			From 				Adobe, 								Mar 07, 2025						                  		From Next Token Prediction to Compliant AI Assistants: A Systematic Path toward Trustworthy Large Language Models	: 		  Language models are systems that can predict upcoming words” - this classical definition of NLP models forms the basis of LLMs becoming responsive text completion models. However, suc. . . 	 			From 				UC Merced, 								Feb 28, 2025						                  		No. 25-01 Show-o: One Single Transformer to Unify Multimodal Understanding and Generation	: 		  Exciting models have been developed in multimodal video understanding and generation, such as video LLM and video diffusion model. One emerging pathway to the ultimate intelligence is. . . 	 			From 				NUS, 								Feb 27, 2025						                  		No. 24-20 Controllable Visual Synthesis via Structural Representation	: 		  End-to-end neural approaches have revolutionized visual generation, producing stunning outputs from natural language prompts. However, precise controls remain challenging through dire. . . 	 			From 				Stanford, 								Dec 13, 2024						                  		No. 24-19 When LLMs Meet Recommendations: Scalable Hybrid Approaches to Enhance User Experiences	: 		  While LLMs offer powerful reasoning and generalization capabilities for user understanding and long-term planning in recommendation systems, their latency and cost hinder direct appli. . . 	 			From 				Deepmind, 								Dec 09, 2024						                  		No. 24-18 Developing Effective Long-Context Language Models	: 		  In this talk, I will share our journey behind developing an effective long-context language model. I’ll begin by introducing our initial approach of using parallel context encoding (C. . . 	 			From 				Princeton, 								Dec 04, 2024						                  		No. 24-17 Mitigating Distribution Shifts in Using Pre-trained Vision-Language Models	: 		  Benefiting from large-scale image-text pair datasets, powerful pre-trained vision-language models (VLMs, such as CLIP) enable many real-world applications, e. g. , zero-shot classificat. . . 	 			From 				UniMelb, 								Dec 02, 2024						                  		No. 24-16 Towards Graph Machine Learning in the Wild	: 		  Learning on graphs is a long-standing and fundamental challenge in machine learning and recent works have demonstrated solid progress in this area. However, most existing models tacit. . . 	 			From 				MIT, 								Nov 27, 2024						                  		No. 24-15 Evaluation and Reasoning in Real-world Scenarios	: 		  User queries in natural settings, such as “provide a design for a disk topology for a NAS built on TrueNAS Scale, as well as a dataset layout,” differ significantly from those produce. . . 	 			From 				Cornell University, 								Nov 20, 2024						                                                &laquo; Prev                              1                               2                               3                               4                               5                               6                              Next &raquo;                                          Upcoming:    				              "
    }, {
    "id": 15,
    "url": "/page3/",
    "title": "UQ Data Science Seminar Series",
    "body": "  {% if page. url ==  /  %}          {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 500px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               From         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. display-date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       From             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. display-date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       From             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. display-date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       From             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. display-date | date: '%b %d, %Y' }}                                {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Talks:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 16,
    "url": "/page4/",
    "title": "UQ Data Science Seminar Series",
    "body": "  {% if page. url ==  /  %}          {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 500px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               From         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. display-date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       From             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. display-date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       From             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. display-date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       From             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. display-date | date: '%b %d, %Y' }}                                {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Talks:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 17,
    "url": "/page5/",
    "title": "UQ Data Science Seminar Series",
    "body": "  {% if page. url ==  /  %}          {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 500px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               From         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. display-date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       From             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. display-date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       From             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. display-date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       From             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. display-date | date: '%b %d, %Y' }}                                {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Talks:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 18,
    "url": "/page6/",
    "title": "UQ Data Science Seminar Series",
    "body": "  {% if page. url ==  /  %}          {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 500px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               From         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. display-date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       From             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. display-date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       From             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. display-date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       From             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. display-date | date: '%b %d, %Y' }}                                {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Talks:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 19,
    "url": "/chuanxiazheng/",
    "title": "Beyond Visual Geometry- Toward Physical 3D Reconstruction",
    "body": "2025/09/16 - Recent progress in 3D Visual Geometry has led to impressive reconstruction and generation results using purely feed-forward deep networks. However, much of this progress remains limited to geometric surface modeling or photorealistic novel view synthesis, often overlooking crucial physical attributes such as occluded geometry, physical stability, and dynamic motion. In this talk, I will present a series of work that go beyond visible geometry. I will introduce Amodal3R, which defines amodal 3D reconstruction from occluded 2D images, predicting complete 3D geometry and appearance when significant portions are hidden from views. Next, I will discuss DSO, which aligns 3D generative models with physics based simulation feedback, ensuring reconstruction are not just visually plausible but physically sound under gravity. To this end, I will introduce Geo4D, to explore how one can build high-quality 4D reconstruction networks starting form video generators pre-trained on millions of videos. [1] Wu, T. , Zheng, C. , Guan, F. , Vedaldi, A. , &amp; Cham, T. J, “Amodal3r: Amodal 3d reconstruction from occluded 2d images”. ICCV 2025. [2] Li, R. , Zheng, C. , Rupprecht, C. , &amp; Vedaldi, A. “DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness”. ICCV 2025. [3] Jiang, Z. , Zheng, C. , Laina, I. , Larlus, D. , &amp; Vedaldi, A. Geo4d: Leveraging video generators for geometric 4d scene reconstruction. ICCV 2025. Speaker Bio: Chuanxia Zheng is a Nanyang Assistant Professor at Nanyang Technological University (NTU), where he leads the physical vision group. Before joining NTU, he is a MSCA fellow in Visual Geometry Group (VGG) at the University of Oxford, working with Andrea Vedaldi on feed-forward photorealistic 3D and 4D reconstruction. He is also a DAAD Ainet Fellow of Germany, a National Research Foundation (NRF) fellow of Singapore. He received his Ph. D. degree from NTU. His research focuses on Creative AI, with the goal of developing systems that can perceive, reconstruct and interact with the physical world. The broader goal is to create realistic digital twins of the physical natural world, capturing diverse physical properties, such as geometry, appearance, occlusion, motion, interaction, gravity, sound and more. More Details:  When: Sep 26 (Friday) 2025, at 1-2pm (Brisbane time) Speaker: Chuanxia Zheng (Nanyang Assistant Professor at NTU) Host: Yujun Cai Zoom: https://uqz. zoom. us/j/89476202411"
    }, {
    "id": 20,
    "url": "/hamid/",
    "title": "AI-Driven Solutions in Individualised Medicine- From Multimodal Omics Data to Disease Diagnosis and Biomarker Discovery",
    "body": "2025/07/12 - Despite significant advancements in medical science and an increasing emphasis on precision medicine, the majority of medical diagnoses are still made after patients exhibit noticeable symptoms. Early diagnosis, detection, and phenotyping of diseases could afford patients and their caregivers the opportunity for timely interventions, potentially leading to improved disease management, better prognoses, and more efficient utilisation of healthcare resources. The rising adoption of personalised medicine by global health systems, coupled with recent advancements in Artificial Intelligence (AI) and Machine Learning (ML) technologies, presents a significant opportunity to leverage AI in personalised medicine. This approach aims to lower the barriers to the clinical integration of personalised medicine, addressing a critical unmet need in the healthcare sector. Dr. Rokny is a multidisciplinary scientist dedicated to employing state-of-the-art AI and ML methodologies in conjunction with multimodal omics data to unravel the mechanisms underlying genetic diseases and cancers. In his presentation, Dr. Rokny will first outline the scope of work undertaken by his Biomedical Machine Learning Laboratory (BML), which spans the application of ML and AI in genomics and spatial transcriptomics to the more recent deep graph representation learning models. He will delve into a specific projects, “TENNIS” and “SemanticST”, offering detailed insights. Additionally, he will discuss the challenges, limitations, and potential biases inherent in the deployment of AI in individualised medicine, refining its role in transforming healthcare. Speaker Bio: Dr. Rokny is a Scientia Senior Lecturer (e. q. , Associate Professor) at UNSW Sydney and Adj Associate Professor at Concordia University, CANADA, obtaining his Ph. D in 2020 at UNSW Sydney. Dr. Rokny is also the Founder and Director of Australian Society of BioMedical Machine Learning with more than 4000 members. He is also the health data analytics program leader at the Macquarie Centre for Applied AI. During his career, he developed a deep cross-disciplinary expertise in Artificial Intelligence (AI), Bio Machine Learning, and Genomics. Dr. Rokny joined UNSW on a highly prestigious and competitive UNSW Scientia Program Fellowship in October 2019 and currently is the Director of UNSW BioMedical Machine Learning laboratory (BML), where he is supervising a group consisting of 20 researchers including HDR and post-doc supervision, further demonstrating his comprehensive leadership and project management capabilities. Dr. Rokny has held continuous peer reviewed salary and grant support for his entire career, including but not limited to two WA Department of Health Merit Fellowships, two UNSW Scientia Fellowship, Tyree Foundation Institute of Health Engineering, and ARC DECRA. Totally, throughout his career, he has received 28 prizes/awards and also was able to secure over $5. 2M peer-reviewed grants as Chief Investigator (CI) including $3. 75M as either sole CI or leading CI. Additionally, Dr. Rokny has produced over 80 peer-reviewed publications in AI method development and their applications in genomics, half of which are in the top 10% of the fields (45 as the leading author), evidencing his impact with over 4,100 citations and a h-index of 36. He has also a strong relationship with industry, in which his expertise in AI and ML has been funded &gt;$10M by his industrial collaborators. Dr. Rokny has also a demonstrated experience in leadership through his contribution to the University and school level committees. He has been invited to serve as keynote speaker and program committee member of national and international conferences (including HUGO 2020) and was also an editorial board member of Communications Biology and Neurocomputing. In addition to his research activities, Dr. Rokny is the course director of AI in Biology, Biomedical Informatics and Digital Medicine at UNSW Sydney. More Details:  When: July 18 2025, at 1-2pm (Brisbane time) Speaker: Hamid Alinejad Rokny (Scientia Senior Lecturer in UNSW) Host: Yujun Cai Venue: 78-411 Zoom: https://uqz. zoom. us/j/83638226838"
    }, {
    "id": 21,
    "url": "/xuefeng_du/",
    "title": "Teach AI What It Doesn't Know",
    "body": "2025/06/17 - The remarkable capabilities of machine learning (ML) models, especially foundation models like GPT, have transformed numerous domains. However, these systems often falter in real-world settings, where they encounter unknown or out-of-distribution (OOD) inputs, and generate overconfident predictions or unreliable outputs. Ensuring their reliability is not only a technical challenge but also a fundamental requirement for their safe deployment. In this talk, I will discuss my research on teaching ML models what they don’t know by developing foundational frameworks for reliable decision-making in the open world. This involves three core aspects: (1) designing novel algorithms for unknown-aware learning through adaptive outlier synthesis, enabling models to handle unfamiliar inputs without explicit knowledge of unknowns; (2) leveraging unlabeled data in the wild to detect and generalize across diverse real-world reliability challenges; and (3) addressing reliability blind spots in foundation models, such as hallucinations, malicious prompts, and noisy alignment data, through innovative mitigation strategies. Through fundamental algorithmic development, theoretical insights, and practical applications, my research contributes to the responsible deployment of AI technologies. The talk will conclude with a forward-looking perspective on interdisciplinary collaborations and the roadmap for achieving robust, reliable AI systems that adapt to an ever-changing world. Speaker Bio: Sean (Xuefeng) Du is an incoming Assistant Professor at College of Computing and Data Science (CCDS), Nanyang Technological University, Singapore. He obtained his Ph. D. in Computer Sciences at UW-Madison advised by Prof. Sharon Li. His research interest is in reliable machine learning and the applications to foundation models and AI safety. His first-author papers have been recognized with multiple oral and spotlight presentations at NeurIPS and CVPR. He is a recipient of the Jane Street Graduate Research Fellowship, and Rising Stars in Data Science award. More Details:  When: June 26 2025, at 1-2pm (Brisbane time) Speaker: Sean (Xuefeng) Du (incoming Assistant Professor in NTU) Host: Yujun Cai Zoom: https://uqz. zoom. us/j/89394692123"
    }, {
    "id": 22,
    "url": "/llm-nick/",
    "title": "Reasoning with Language Models",
    "body": "2025/05/25 - Language models are primarily trained via imitation on massive amounts of human data; as a result, they’re capable of performing a wide range of tasks but often lack the deep reasoning capabilities of classic AI systems like Deep Blue and AlphaGo. In this talk, I’ll first present core technical challenges related to “reasoning with language,” using my work on computer crossword solvers as a running example. Then, I’ll show how methods for “interactive reasoning” can enable human-AI teams to solve complex problems jointly. Finally, I’ll discuss my work on “explainable reasoning,” where the goal is to explain the decisions made by expert AI systems like AlphaGo in human-interpretable terms. I will conclude by sharing my views on the future of language model reasoning, agents, and interactive systems. Speaker Bio: Nicholas Tomlin is a final-year PhD student in the Berkeley NLP Group, where he is advised by Dan Klein. His work focuses primarily on reasoning and multi-agent interaction with language models. He has co-created systems such as The Berkeley Crossword Solver, the first superhuman computer crossword solver, as well as Ghostbuster, a state-of-the-art method for LLM detection. His work has been supported by grants from the NSF and FAR AI and has received media coverage from outlets such as Discover, Wired, and the BBC. He is also an incoming CDS Faculty Fellow at NYU and incoming assistant professor at TTIC. More Details:  When: May 29 2025, at 1-2pm (Brisbane time) Speaker: Nicholas Tomlin (UC Berkeley) Host: Dr Ruihong Qiu Zoom: https://uqz. zoom. us/j/86460266783"
    }, {
    "id": 23,
    "url": "/cv-qianyiwu/",
    "title": "Towards Efficient Novel View Synthesis",
    "body": "2025/05/17 - Novel view synthesis (NVS) from 2D images aims to generate unseen views of a scene given multiple input observations. It is a fundamental task in computer vision that has garnered significant attention due to recent advances in 3D representations and neural rendering. Techniques such as Neural Radiance Fields and 3D Gaussian Splatting have substantially improved NVS quality, yet the demand for more efficient approaches—in terms of space, time, and storage—remains a critical research direction. In this talk, I will present our recent efforts to address these challenges. Specifically, I will discuss two of our latest works: FCGS (ICLR 2025), which introduces a compression method for 3D Gaussian Splatting, and Pansplat (CVPR 2025), a feed-forward model designed for panoramic novel view synthesis. Speaker Bio: Qianyi Wu is a final-year PhD at Department of Data Science and AI, Monash University, under the supervision of Prof. Jianfei Cai. Qianyi received B. S. degree in Special Class for the Gifted Youth at University of Science and Technology of China (USTC) in 2016. He received M. Sc degree from Graphics and Geometric Computing Laboratory of the School of Mathematical Sciences at USTC in 2019, under the supervision of Prof. Juyong Zhang. He worked as a research scientist intern at Meta Reality Lab in 2024. His recent research area focuses on 3D reconstruction and generation. More Details:  When: May 23 2025, at 3-4pm (Brisbane time) Speaker: Qianyi Wu (Monash University) Host: Dr Yujun Cai Zoom: https://uqz. zoom. us/j/88094383147"
    }, {
    "id": 24,
    "url": "/ds-yupenghou/",
    "title": "Towards Large Generative Recommendation Models: A Tokenization Perspective",
    "body": "2025/05/05 - The emergence of large generative models is transforming the landscape of recommender systems, offering new opportunities through scaling laws and flexible content generation. One of the most fundamental components is action tokenization, which is the process of converting human-readable data (e. g. , text or user-item interactions) into model-readable token sequences. This talk provides a tokenization-centric view of building effective and efficient generative recommendation models. We begin by introducing the backgrounds of semantic IDs and action tokenization techniques, as covered in our WWW’25 tutorial. We then introduce two recent work: aligning action tokens with LLMs (ICDE’24) and contextually tokenizing action sequences (ICML’25 spotlight). Speaker Bio: Yupeng Hou is a Ph. D. student at the University of California, San Diego, advised by Prof. Julian McAuley. He was a student researcher at Google DeepMind in 2024, working with Jianmo Ni, Derek Cheng, and Ed H. Chi. He previously received his M. E. and B. E. from Renmin University of China, advised by Prof. Wayne Xin Zhao. His work has been recognized as the Best Resource Paper Runner-up at CIKM 2022 and the Best Student Paper Runner-up at RecSys 2022. Yupeng is one of the leading developers of RecBole, a popular open-source recommendation library with over 3,700 GitHub stars. His current research focuses on generative recommendation and tokenization. More Details:  When: Thursday. 22 May 2025, at 1-2pm (Brisbane time) Speaker: Yupeng Hou (UCSD) Host: Dr Ruihong Qiu Zoom: https://uqz. zoom. us/j/86490391734"
    }, {
    "id": 25,
    "url": "/gnn-fangyuan/",
    "title": "Would prompt work for graph learning? An exploration of few-shot learning on graphs",
    "body": "2025/04/11 - Graph structures are prevalent across a variety of fields, including social networks, e-commerce, transportation, and biological systems. Within these graphs, numerous analytical and mining tasks can be identified, often aligning with link prediction, node classification, or graph classification. Graph Neural Networks (GNNs) have achieved significant success in these applications, primarily due to their capability to learn powerful graph representations. Nevertheless, the efficacy of GNNs often depends on the availability of labeled data, without which their performance may be compromised. This talk seeks to delve into learning paradigms that diverge from traditional supervised learning, in the context of few-shot learning on graphs. In particular, drawing inspiration from recent progress in language modeling, we pose an intriguing question: Can prompt-based learning be effectively adapted for graph data? Our talk will begin with a comprehensive overview of few-shot learning methodologies on graphs, followed by highlighting some of our representative works in this area. Speaker Bio: Dr. Yuan Fang is a tenure-track Assistant Professor at the School of Computing and Information Systems at Singapore Management University (SMU). He was previously a data scientist at DBS Bank and a research scientist at A*STAR. His research interests revolve around graph learning and its applications in recommender systems, social network analysis, and science for AI. Dr. Fang has published over 90 papers in leading conferences and journals, and his work has been featured as “Most Influential Paper in WWW’23” (PaperDigest, Sep 2024), as well as “Best Papers of VLDB13” (VLDB Journal Special Issue). He is a Young Associate Editor of Frontiers of Computer Science (Springer &amp; Higher Education Press), and serves various international conferences as organization committee member and area chair. More Details:  When: Thursday. 17 April 2025, at 1-2pm (Brisbane time) Speaker: Prof Yuan Fang (SMU) Host: Dr Yujun Cai Zoom: https://uqz. zoom. us/j/84511228842"
    }, {
    "id": 26,
    "url": "/knowllm-heng/",
    "title": "Towards Knowledgeable Foundation Models",
    "body": "2025/04/07 - Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance on knowledge reasoning tasks, owing to their implicit knowledge derived from extensive pretraining data. However, their inherent knowledge bases often suffer from disorganization and illusion, bias towards common entities, and rapid obsolescence. Consequently, LLMs frequently make up untruthful information, exhibit resistance to updating outdated knowledge, or struggle with generalizing across multiple languages. In this talk I will discuss several research directions that aim to make foundation models’ knowledge more accurate, organized, up-to-date and fair: (1) Where and How is Knowledge Stored in LLM? (2) How to Control LLM’s Knowledge? (3) How to Update LLM’s Dynamic Knowledge? (4) How to Bridge the Knowledge Gap between Natural Language and Unnatural Language? Speaker Bio: Heng Ji is a Tenured Full Professor and Associate Director for Research of Siebel School of Computing and Data Science, and a faculty member affiliated with Electrical and Computer Engineering Department, Coordinated Science Laboratory, and Carl R. Woese Institute for Genomic Biology of University of Illinois Urbana-Champaign. She is an Amazon Scholar. She is the Founding Director of Amazon-Illinois Center on AI for Interactive Conversational Experiences (AICE). She received Ph. D. in Computer Science from New York University. Her research interests focus on Natural Language Processing, especially on Multimedia Multilingual Information Extraction, Knowledge-enhanced Large Language Models and Vision-Language Models, and AI for Science. The awards she received include Outstanding Paper Award at ACL2024, two Outstanding Paper Awards at NAACL2024, “Young Scientist” by the World Laureates Association in 2023 and 2024, “Young Scientist” and a member of the Global Future Council on the Future of Computing by the World Economic Forum in 2016 and 2017, “Women Leaders of Conversational AI” (Class of 2023) by Project Voice, “AI’s 10 to Watch” Award by IEEE Intelligent Systems in 2013, NSF CAREER award in 2009, PACLIC2012 Best paper runner-up, “Best of ICDM2013” paper award, “Best of SDM2013” paper award, ACL2018 Best Demo paper nomination, ACL2020 Best Demo Paper Award, NAACL2021 Best Demo Paper Award, Google Research Award in 2009 and 2014, IBM Watson Faculty Award in 2012 and 2014 and Bosch Research Award in 2014-2018. She served as the associate editor for IEEE/ACM Transaction on Audio, Speech, and Language Processing, and the Program Committee Co-Chair of many conferences including NAACL-HLT2018 and AACL-IJCNLP2022. She was elected as the North American Chapter of the Association for Computational Linguistics (NAACL) secretary 2020-2023. More Details:  When: Thu. 10 April 2025, at 1-2pm (Brisbane time) Speaker: Prof Heng Ji (University of Illinois Urbana-Champaign) Host: Dr Ruihong Qiu Zoom: https://uqz. zoom. us/j/86395470812 (No recording will be provided)"
    }, {
    "id": 27,
    "url": "/cv-chizhang/",
    "title": "AppAgent X—Making GUI Agents Smarter with Use",
    "body": "2025/03/21 - In recent years, the development of multimodal large language models has given rise to a new class of intelligent agents—GUI Agents—that can autonomously operate computers and smartphones through natural language, enabling automated office tasks and cross-application task execution. However, existing methods rely on step-by-step reasoning, resulting in high computational costs and low execution efficiency, especially for repetitive tasks. To overcome this bottleneck, we introduce AppAgent X, an evolutionary GUI agent framework. Unlike traditional agents that repeatedly reason through each step, AppAgent X learns from its own operational experience, continuously generalizing and optimizing efficient behavior patterns. This enables the agent to become more efficient and intelligent with use. This presentation will cover the core mechanisms of AppAgent X, experimental results, and its potential applications in the field of intelligent agents. Speaker Bio: Dr. Chi Zhang received his Ph. D. from the School of Computer Science and Engineering at Nanyang Technological University, Singapore. In 2024, he joined the School of Engineering at Westlake University as an Assistant Professor (PI, Ph. D. Supervisor) and founded the Artificial General Intelligence (AGI) Lab. Prior to this, he worked as a Research Scientist at Tencent from 2022 to 2024. Dr. Zhang’s research focuses on multimodal models and generative artificial intelligence (GenAI). To date, he has published over 30 papers in top-tier AI conferences and journals, including CVPR, ICCV, NeurIPS, and TPAMI. He was named among the World’s Top 2% Scientists by Stanford University in both 2023 and 2024. More Details:  When: Wed. 2 April 2025, at 1-2pm (Brisbane time) Speaker: Prof Chi Zhang (Westlake University) Host: Dr Yujun Cai Zoom: https://uqz. zoom. us/j/88065580162"
    }, {
    "id": 28,
    "url": "/nlp-Douglasu/",
    "title": "Using Large Language Models for Cross-Language Information Access",
    "body": "2025/03/14 - One interesting aspect of today’s generative Large Language Models (LLMs) is that they are natural polyglots, facile in many languages.    These new multi-dexterous capabilities offer new potential for helping people to transcend language barriers when seeking information.   In this talk I will describe three projects from our group at the Johns Hopkins University Human Language Technology Center of Excellence (HLTCOE) in which we have been exploring that potential.   In the first, we explore the use of LLMs to train ColBERT to optimize ranking quality for Cross-Language Information Retrieval (CLIR).   This training process benefits from large quantities of positive query-passage pairs, so the question we ask is whether generative LLM’s can generate such pairs from the collection that is ultimately to be searched, with the queries in some desired language that is different from the language in the collection.   In the second, we explore whether LLMs can help to characterize the ranking quality of a CLIR system by developing test collections for new language pairs.   Here our approach is use an LLM to approximate the relevance judgments that a human assessor would have made.   In the third, we move beyond retrieval to the direct use of generation, exploring the potential of LLMs to write a report on a topic using source materials in multiple languages, looking both at how we might do this, and how we might know if we have done it well.   This will be the focus of the new TREC RAGTIME track and a 10-week workshop at the HLTCOE this summer.   This is joint work with Noah Hibbler, Efsun Kayi, Reno Kriz, Dawn Lawrie, Sean MacAvaney, Marc Mason, Jim Mayfield, Paul McNamee, Scott Miller, Ian Soboroff, Kate Sanders, Luca Soldaini, Paul Thomas, Will Walden, Orion Weller, Eugene Yang and Andrew Yates. Speaker Bio: Doug Oard is a Professor at the University of Maryland, with joint appointments in the College of Information (the iSchool) and the University of Maryland Institute for Advanced Computer Studies (UMIACS), and an affiliate appointment at the Johns Hopkins University Human Language Technology Center of Excellence.  He is perhaps best known for his research on cross-language information retrieval, but more generally one thread of his research has addressed the use of technologies such as machine translation, speech recognition, document image analysis, knowledge representation, processing mathematical notation, and social network analysis to support information access. He also has interests in applications of information retrieval in specific settings, including archival access and the “discovery” process for exchanging evidence among parties to civil litigation. Among his other current projects are (1) leveraging multiple sources of evidence to help people find content in archives that has not yet been digitized or well described, and (2) detecting inference risks when reviewing previously restricted materials for declassification. More information on Doug’s research and teaching can be found at http://terpconnect. umd. edu/~oard More Details:  When: Fri. 28 Mar 2025, at 1-2pm (Brisbane time) Speaker: Professor Douglas W. Oard, University of Maryland (USA) Host: Dr Joel Mackenzie Venue: 50-N202 - Hawken Engineering Building, Learning Theatre Zoom: https://uqz. zoom. us/my/joelmackenzie"
    }, {
    "id": 29,
    "url": "/nlp-Julien/",
    "title": "Leveraging semantics for recommendation at scale",
    "body": "2025/03/14 - In this talk, we present some of our recent work conducted at Amazon International Machine Learning Australia. First, we present a simple approach to address cold-start recommendation by leveraging semantic information (ACM Recsys ’24). We then introduce a notion of generalization gap in collaborative filtering, and derive a geometric upper bound and a way to meaningfully utilize the geometry of the product metadata to improve recommendations (AISTATS ’25).  We finally present an application of sequential recommendation for complementary product selection (ACM WSDM ’25). Speaker Bio: Dr Julien Monteil is leading the Machine Learning group at Amazon International Machine Learning Australia, which primarily focuses on the Research and Development of recommender systems for Amazon customers globally. He is also an Adjunct Senior Lecturer at the University of Queensland, School of Civil Engineering. He has 12 years of post-PhD experience in the Research and Development of ML systems for customer-facing applications in retail, automotive, networking, healthcare. He developed and launched dozens of ML systems for Amazon, AWS, IBM, including many systems benefiting millions of customers monthly, and generating dozens of millions yearly in business impact. He authored 50+ peer-reviewed papers and 20+ patents in diverse research communities, including optimization and control, transportation, while now actively contributing to the Machine Learning and Recommender Systems communities. More Details:  When: Wed. 26 Mar 2025, at 11am. -12pm (Brisbane time) Speaker: Dr Julien Monteil, Amazon International Machine Learning (Australia) Host: Dr Rocky Chen Venue: 14-217 Zoom: https://uqz. zoom. us/j/87431037194"
    }, {
    "id": 30,
    "url": "/nlp-jiuxianggu/",
    "title": "Towards Multimodal Intelligence: Bridging Vision, Language, and Large-Scale Models",
    "body": "2025/03/02 - Multimodal intelligence is revolutionizing document understanding by enabling AI to process and reason across vision and language. This talk explores how large-scale models integrate textual and visual information to enhance document analysis, addressing key challenges such as multimodal fusion, layout-aware learning, and post-training optimization. We discuss strategies to improve model adaptability, including contrastive learning and retrieval-augmented fine-tuning. Finally, we highlight future directions in cross-modal reasoning and scalable architectures, aiming to advance automation and knowledge extraction in AI-driven document intelligence. Speaker Bio: Dr. Jiuxiang Gu is a Senior Research Scientist at Adobe Research, focusing on Machine Learning Theory and Multimodal Learning. He earned his Ph. D. from Nanyang Technological University, Singapore, and has served as an Area Chair for ICLR, ACL, and WACV, alongside various program committee roles. Recognized in Stanford University’s 2023/24 list of the top 2% of scientists globally, Dr. Gu has extensive experience in integrating multimodal large models with interdisciplinary applications, particularly in processing multimodal document data. Visit his personal homepage at gujiuxiang. com More Details:  When: Fri. 07 Mar 2025, at 10 - 11 am (Brisbane time) Speaker: Dr Jiuxaing Gu (Adobe) Host: Dr Yujun Cai Zoom: https://uqz. zoom. us/j/86590016543 [Recording will only be available internally by request. ]"
    }, {
    "id": 31,
    "url": "/nlp-yiweiwang/",
    "title": "From Next Token Prediction to Compliant AI Assistants: A Systematic Path toward Trustworthy Large Language Models",
    "body": "2025/02/23 - Language models are systems that can predict upcoming words” - this classical definition of NLP models forms the basis of LLMs becoming responsive text completion models. However, such “responsive” capability is not good enough for LLMs to serve as “compliant AI assistants”. This talk will address the gap between current LLMs and “compliant AI assistants” that limits the broad real-world applications of LLMs. The speaker will introduce a systematic research path to develop trustworthy LLMs as compliant AI assistants. The talk will mainly cover the speaker’s research on controllable generation, context faithfulness, and safety of LLMs. The research methodology will cover advanced techniques in training, data synthesis, prompt engineering, decoding, and meta-generation that comprehensively develop trustworthy LLM-based AI systems. Speaker Bio: Yiwei Wang is currently an Assistant Professor at the Computer Science Department of University of California, Merced. He leads the UC Merced NLP Lab.  Yiwei Wang worked as a Postdoc in UCLA NLP Lab, an Applied Scientist in Amazon Inc. (Seattle). He received his Ph. D. in Computer Science at National University of Singapore in 2023, where he was fortunate to be advised by Prof. Bryan Hooi. His current research is focused on natural language processing, and especially interested in building trustworthy AI assistants to provide responsible services to humans in real-world applications. Yiwei Wang is actively recruiting strong and motivated students as Ph. D. students or research interns. Please feel free to email him at yiweiwang2@ucmerced. edu if you are interested. More Details:  When: Fri. 28 Feb 2025, at 10 - 11 am (Brisbane time) Speaker: Prof Yiwei Wang (UC Merced) Host: Dr Yujun Cai Venue: 78-420 Zoom: https://uqz. zoom. us/j/81297131414 [Recording will only be available internally by request. ]"
    }, {
    "id": 32,
    "url": "/showo-mikeshou/",
    "title": "No.25-01 Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
    "body": "2025/02/21 - Exciting models have been developed in multimodal video understanding and generation, such as video LLM and video diffusion model. One emerging pathway to the ultimate intelligence is to create one single foundation model that can do both understanding and generation. After all, humans only use one brain to do both tasks. Towards such unification, recent attempts employ a base language model for multimodal understanding but require an additional pre-trained diffusion model for visual generation, which still remain as two separate components. In this work, we present Show-o, one single transformer that handles both multimodal understanding and generation. Unlike fully autoregressive models, Show-o is the first to unify autoregressive and discrete diffusion modeling, flexibly supporting a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation of any input/output format, all within one single 1. 3B transformer. Across various benchmarks, Show-o demonstrates comparable or superior performance, shedding light for building the next-generation video foundation model. Speaker Bio: Mike Shou is a tenure-track Assistant Professor (Presidential Young Professorship) at National University of Singapore. He was a Research Scientist at Facebook AI in Bay Area. He obtained his Ph. D. degree at Columbia University in the City of New York, working with Prof. Shih-Fu Chang. He received the Best Paper Finalist at CVPR’22, Best Student Paper Nomination at CVPR’17, PREMIA Best Paper Award 2023, EgoVis Distinguished Paper Award 2022/23. His team won the 1st place in the international challenges including EPIC-Kitchens 2022, Ego4D 2022 &amp; 2023. He is a Singapore Technologies Engineering Distinguished Professor and a Fellow of National Research Foundation Singapore. He is on the Forbes 30 Under 30 Asia list. More Details:  When: Thu 27 Feb 2025, at 1 - 2 pm (Brisbane time) Speaker: Prof Mike Shou (NUS) Host: Dr Ruihong Qiu Zoom: https://uqz. zoom. us/j/86003984130 [Recording will only be available internally by request. ]"
    }, {
    "id": 33,
    "url": "/synt-yunzhi/",
    "title": "No.24-20 Controllable Visual Synthesis via Structural Representation",
    "body": "2024/12/06 - End-to-end neural approaches have revolutionized visual generation, producing stunning outputs from natural language prompts. However, precise controls remain challenging through direct text-to-scene generation, as natural language alone lacks the precision needed for specifying complex visual relationships. This talk explores compositional visual representations bridging pre-trained language and visual generative models through programs for scene structure, words for semantic abstraction, and neural embeddings capturing visual identity. Our results show how such representations enable precise scene control while building on modern generative models’ capabilities, suggesting a scalable path for controllable visual synthesis. Speaker Bio: Yunzhi Zhang is a PhD student at Stanford University, advised by Jiajun Wu. Her current research interest lies in learning structural representation and generative models from visual data. She is supported by the Stanford Interdisciplinary Graduate Fellowship. More Details:  When: Fri 13 Dec 2024, at 1 - 2 pm (Brisbane time) Speaker: Yunzhi Zhang (Stanford) Host: Ruihong Qiu Zoom: https://uqz. zoom. us/j/83905648246 [Recording]"
    }, {
    "id": 34,
    "url": "/llmrec-jianling/",
    "title": "No.24-19 When LLMs Meet Recommendations: Scalable Hybrid Approaches to Enhance User Experiences",
    "body": "2024/12/02 - While LLMs offer powerful reasoning and generalization capabilities for user understanding and long-term planning in recommendation systems, their latency and cost hinder direct application in large-scale industrial settings. The talk will cover our recent work on scalable hybrid approaches that combine LLMs and traditional recommendation models. We’ll explore their effectiveness in tackling challenges like cold-start recommendations and enhancing user exploration. Speaker Bio: Jianling Wang is a senior research scientist working at Google DeepMind. She obtained her Ph. D. degree from the Department of Computer Science and Engineering at Texas A&amp;M University, advised by Prof. James Caverlee. Her research interests generally include data mining and machine learning, with a particular focus on recommendation systems and graph neural networks. More Details:  When: Mon 9 Dec 2024, at 1 - 2 pm (Brisbane time) Speaker: Dr Jianling Wang (Google Deepmind) Host: Dr Ruihong Qiu Zoom: https://uqz. zoom. us/j/83289875914 [No recording will be provided. ]"
    }, {
    "id": 35,
    "url": "/ds-lclm-tianyu/",
    "title": "No.24-18 Developing Effective Long-Context Language Models",
    "body": "2024/11/28 - In this talk, I will share our journey behind developing an effective long-context language model. I’ll begin by introducing our initial approach of using parallel context encoding (CEPE). Despite achieving significant efficiency gain and promising perplexity, we found that CEPE often struggled on basic synthetic tasks. This motivated us to create HELMET, a comprehensive and reliable benchmark for long-context LMs, featuring a diverse range of tasks and developer-friendly features. Using HELMET and insights from CEPE, we conducted extensive ablation studies on data mixture, scaling, instruction tuning, and position extrapolation. This led to ProLong, a top-performing 10B long-context model with an effective context length of 512K. Speaker Bio: Tianyu Gao is a 5th-year PhD student at Princeton University advised by Prof. Danqi Chen. He is interested in building capable, reliable, and efficient language models, with a current focus on long-context LMs and instruction following. He co-organized the first workshop on long-context foundation models at ICML 2024. He received an IBM PhD Fellowship in 2023 and an ACL Outstanding Paper Award in 2022. More Details:  When: Wed 4 Dec 2024, at 1 - 2 pm (Brisbane time) Speaker: Tianyu Gao (Princeton University) Host: Ruihong Qiu Zoom: https://uqz. zoom. us/j/84022245453 [Recording]"
    }, {
    "id": 36,
    "url": "/ds-vlm-feng/",
    "title": "No.24-17 Mitigating Distribution Shifts in Using Pre-trained Vision-Language Models",
    "body": "2024/11/24 - Benefiting from large-scale image-text pair datasets, powerful pre-trained vision-language models (VLMs, such as CLIP) enable many real-world applications, e. g. , zero-shot classification and image-text retrieval. However, for many real-world applications, their datasets have different data distributions from the dataset used to train VLMs, might causing poor performance (based on machine learning theory) when we use these VLMs. To mitigate the negative effects brought from these shifts, we normally try to 1) fine-tune a pre-trained VLM with downstream tasks or 2) further improve the generalisation ability of a pre-trained VLM. In this talk, I will first introduce our recent work (one oral paper and one poster paper in ICML 2024) in both directions of 1) and 2). Then, I will introduce another work on how to detect label set shift when using pre-trained VLMs in zero-shot classification (one spotlight paper in ICLR 2024). Speaker Bio: Dr Feng Liu is a machine learning researcher with research interests in hypothesis testing and trustworthy machine learning. Currently, he is the recipient of the ARC DECRA Fellowship, a Lecturer at The University of Melbourne, Australia, and a Visiting Scientist at RIKEN-AIP, Japan. He has served as an Area Chair for ACM MM, AISTATS, ICLR, ICML, NeurIPS, as a senior program committee (SPC) member for AAAI, IJCAI, ECAI. He has received the ARC Discovery Early Career Researcher Award, the Outstanding Paper Award of NeurIPS (2022), the Outstanding Area Chair Award of ACM MM (2024), the Outstanding Reviewer Award of NeurIPS (2021), and the Outstanding Reviewer Award of ICLR (2021). More Details:  When: Mon 2 Dec 2024, at 11 am - 12 pm (Brisbane time) Speaker: Dr Feng Liu (The University of Melbourne) Host: Prof Helen Huang Venue: 78-420 Zoom: https://uqz. zoom. us/j/86204540916 [Recording]"
    }, {
    "id": 37,
    "url": "/gnn-wild-qitian/",
    "title": "No.24-16 Towards Graph Machine Learning in the Wild",
    "body": "2024/11/20 - Learning on graphs is a long-standing and fundamental challenge in machine learning and recent works have demonstrated solid progress in this area. However, most existing models tacitly assume a closed-world setting where the training and testing data are sampled from an identical distribution. In this talk, we will introduce recent advances that develop theoretically principled and practical useful methods for learning on graphs in the challenging open-world hypothesis, where the model needs to generalize to out-of-distribution testing data in the wild. From the methodological view, we will present two technical paths for building provably generalizable learning algorithms, based on invariance and causality principles, respectively. On the applicable side, we will discuss how to apply these methods to address the pressing problems in recommender systems and molecular discovery. Speaker Bio: Qitian Wu is a postdoctoral fellow at Broad Institute of MIT and Harvard. Prior to this, he achieved PhD in Computer Science at Shanghai Jiao Tong University. His research interest focuses on machine learning with complex structured data. His recent works endeavor to develop efficient foundational backbones for representing large-scale graph data and provably generalizable learning algorithms for handling distribution shifts. He also seek to apply this methodology to address the pressing problems in recommender systems and biomedical science. He is the recipient of Eric and Wendy Schmidt Center Fellowship, Microsoft Research Fellowship, Baidu Scholarship and Rising Star in Artiﬁcial Intelligence. More Details:  When: Wed 27 Nov 2024, at 1 - 2 pm (Brisbane time) Speaker: Dr Qitian Wu (MIT &amp; Harvard) Host: Dr Ruihong Qiu Venue: Online Zoom: https://uqz. zoom. us/j/85331921600 [Recording]"
    }, {
    "id": 38,
    "url": "/eval-wenting/",
    "title": "No.24-15 Evaluation and Reasoning in Real-world Scenarios",
    "body": "2024/11/15 - User queries in natural settings, such as “provide a design for a disk topology for a NAS built on TrueNAS Scale, as well as a dataset layout,” differ significantly from those produced in crowdsourced environments, like “Did Aristotle use a laptop?” Addressing real-world user queries requires language models (LMs) to possess more comprehensive abilities, such as resolving ambiguity, applying world knowledge, and reasoning within context. This talk will cover evaluating and training LMs to reason in real-world scenarios. Speaker Bio: Wenting Zhao is a final-year Ph. D. candidate in Computer Science at Cornell University, advised by Professors Claire Cardie and Professor Sasha Rush. Her research focuses on reasoning: she develops techniques that can accurately reason over real-world scenarios and creates benchmarks that reliably reflect the reasoning performance of language models when deployed in the real world. She was named the intern of the year at AI2, and she organized reasoning tutorials and workshops at ACL conferences. More Details:  When: Wed 20 Nov 2024, at 1 - 2 pm (Brisbane time) Speaker: Wenting Zhao (Cornell University) Host: Ruihong Qiu Venue: Online Zoom: https://uqz. zoom. us/j/82860997780"
    }, {
    "id": 39,
    "url": "/longrangerecsys-jiahao/",
    "title": "No.24-14 Long-Range Meets Scalability: Unveiling a Linear-Time Graph Neural Network for Recommendation at Scale",
    "body": "2024/11/06 - Recommender systems play a central role in shaping our daily digital experiences, yet achieving both scalability and expressive power remains a significant challenge. While Graph Neural Networks (GNNs) excel at capturing long-range user-item relationships, their heavy computational demands often make them less practical at large scales, where simpler models like Matrix Factorization (MF) and Deep Neural Networks (DNNs) are more commonly used. In this talk, I will introduce Linear-Time Graph Neural Network (LTGNN), a breakthrough approach designed to bridge the scalability gap between GNNs and simpler methods without sacrificing GNNs’ unique ability to capture distant dependencies. Through a fixed-point formulation, LTGNN reduces the number of GNN layers to one by reusing historical fixed-point computations, preserving a large “receptive field” on graphs. Additionally, a variance reduction mechanism accelerates neighbor aggregation, achieving high accuracy with a minimal number of sampled neighbors. Together, these two innovations enable LTGNN to operate at near-MF complexity, as verified by extensive empirical results. While this talk covers our recent WWW 2024 paper, it also goes beyond by introducing new theoretical insights, deeper discussions, and foundational design principles. This presentation will highlight a promising direction for scaling up GNNs effectively in recommendation systems. Speaker Bio: Jiahao Zhang is currently a first-year PhD student at the Pennsylvania State University, advised by Prof. Suhang Wang. He was previously has an M. Phil. student at The Hong Kong Polytechnic University, advised by Prof. Wenqi Fan and Prof. Qing Li. He is interested in the scalability and trustworthiness of graph neural networks, as well as the application of graph neural networks on recommender systems. More Details:  When: Wed 13 Nov 2024, at 1 - 2 pm (Brisbane time) Speaker: Jiahao Zhang (Pennsylvania State University) Host: Ruihong Qiu Venue: Online Zoom: https://uqz. zoom. us/j/87200611980 [Recording]"
    }, {
    "id": 40,
    "url": "/context-jack/",
    "title": "No.24-13 Contextual Document Embeddings",
    "body": "2024/11/02 - Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval, and that a contextualized document embedding should take into account both the document and neighboring documents in context - analogous to contextualized word embeddings. We propose two complementary methods for contextualized document embeddings: first, an alternative contrastive learning objective that explicitly incorporates the document neighbors into the intra-batch contextual loss; second, a new contextual architecture that explicitly encodes neighbor document information into the encoded representation. Results show that both methods achieve better performance than biencoders in several settings, with differences especially pronounced out-of-domain. We achieve state-of-the-art results on the MTEB benchmark with no hard negative mining, score distillation, dataset-specific instructions, intra-GPU example-sharing, or extremely large batch sizes. Our method can be applied to improve performance on any contrastive learning dataset and any biencoder. arXiv Preprint: Contextual Document Embeddings Speaker Bio: Jack (John) Morris is a PhD student at Cornell University and researcher at Meta AI (previously FAIR). Before joining Cornell, he was a Google AI Resident. He works on understanding and improving text embeddings. More Details:  When: Wed 06 Nov 2024, at 11:00 am - 12:00 pm (Brisbane time) Speaker: Jack (John) Morris (Cornell University &amp; Meta AI) Host: Ruihong Qiu Venue: Online Zoom: https://uqz. zoom. us/j/87626530209 [Recording]"
    }, {
    "id": 41,
    "url": "/epi-wei/",
    "title": "No.24-12 Graph Neural Networks in Epidemic Modeling: An In-Depth Review and Toolkit",
    "body": "2024/10/22 - Since the onset of the COVID-19 pandemic, there has been growing interest in epidemic modeling. While traditional mechanistic models effectively describe the mathematical dynamics of disease transmission, they often face challenges in handling complex, real-world scenarios. In contrast, Graph Neural Networks (GNNs) have emerged as a powerful alternative in epidemic research. This talk offers a comprehensive review of GNN applications in epidemic modeling, presenting a hierarchical taxonomy for both epidemiological tasks and modeling techniques. We categorize methods into Neural Models and Hybrid Models and introduce our Python toolkit, EpiLearn, which encompasses a wide range of these approaches. This talk will also cover the limitations of current models from multiple perspectives and propose future research directions. By providing a thorough exploration of existing GNN models, we aim to equip researchers with valuable insights into the current state and future possibilities of using GNNs in epidemiology. Speaker Bio: Wei Jin is an Assistant Professor of Computer Science at Emory University. His research focuses on graph machine learning and time series analysis, with notable accomplishments such as INNS Doctoral Dissertation Runner-up Award, KAUST Rising Stars in AI, AAAI New Faculty Highlights, Most Influential Papers in KDD and WWW by Paper Digest, and top finishes in three NeurIPS competitions. He has led teams in building well-received open-source machine learning platforms including EpiLearn (https://github. com/Emory-Melody/EpiLearn), the first Python toolkit for machine learning in epidemic modeling. In addition, he has organized multiple tutorials and workshops at top conferences, and published in top-tier venues such as ICLR, KDD, ICML, and NeurIPS. He has served as (senior) program committee members at these conferences and received the WSDM Outstanding Program Committee Member award. More Details:  When: Wed 30 Oct 2024, at 1:00 - 2:00 pm (GMT+10) Speaker: Prof Wei Jin (Emory University) Host: Dr Ruihong Qiu Venue: Online Zoom: https://uqz. zoom. us/j/89353235547 [Recording]"
    }, {
    "id": 42,
    "url": "/3d-elliott/",
    "title": "No.24-10 Recreating the Physical Natural World from Images",
    "body": "2024/10/01 - Today, generative AI models excel at creating visual worlds through pixels, but still often struggle with the comprehension of basic physical concepts such as 3D shape, motion, material, and lighting—key elements that connect computer vision to a wide range of engineering disciplines for building real world applications, including interactive VR, robotics, and scientific analysis. A major roadblock has been the difficulty in collecting large-scale datasets of physical measurements for training. In this talk, I will discuss an alternative approach through inverse rendering, which enables machine learning models to extract explicit physical representations from raw, unstructured image data, such as Internet photos and videos. This approach thus circumvents the need for any direct physical measurements, allowing us to model a wide variety of 3D objects in nature, including diverse wildlife, using only casually recorded imagery. The resulting model turns images into physically-grounded 3D assets and controllable animations instantly, ready for downstream rendering and analysis. Speaker Bio: Shangzhe Wu is a postdoc researcher at Stanford University working with Jiajun Wu. He will join the Department of Engineering at the University of Cambridge in spring 2025. He received his PhD from the University of Oxford, advised by Andrea Vedaldi and Christian Rupprecht. His research focuses on 3D computer vision and inverse rendering. His work received the Best Paper Award at CVPR 2020, the BMVA Sullivan Doctoral Thesis Prize and the ECVA PhD Award. More Details:  When: Wed 09 Oct 2024, at 1:00 - 2:00 pm (GMT+10, Brisbane time) Speaker: Dr Shangzhe Wu (Stanford University) Host: Dr Ruihong Qiu Venue: Online Zoom: https://uqz. zoom. us/j/85677992148 [Recording]"
    }, {
    "id": 43,
    "url": "/vlnllm-qi/",
    "title": "No.24-09 Human-Computer Conversational Vision-and-Language Navigation",
    "body": "2024/09/24 - The dynamic realm of Vision-and-Language Navigation (VLN) has garnered significant multidisciplinary interest, resonating within the domains of computer vision, natural language processing, and robotics. This presentation embarks on a comprehensive exploration of the VLN trajectory, tracing its inception to seminal benchmarks such as Room-to-Room (R2R). A pivotal catalyst within this evolution is the advent of Large Language Models (LLMs), exemplified by the transformative GPT-4. These LLMs have not only facilitated more natural and fluid human-machine interactions but also unlocked novel pathways for leveraging human-like language in guiding robots through intricate navigational tasks. The discourse commences by establishing the contextual framework of human-machine conversational dynamics, contextualizing the paradigm shift and its reverberations. Subsequently, a detailed exposition of our recent undertakings in the VLN domain is presented. This involves harnessing the prowess of LLMs to decode complex navigational instructions embedded within natural language, thereby elevating robotic navigational capabilities. The presentation serves as an illuminating window into the transformative potential of merging vision, language, and robotics. Speaker Bio: Dr Qi Wu is an Associate Professor at the University of Adelaide and was the ARC Discovery Early Career Researcher Award (DECRA) Fellow between 2019-2021. He is the Director of Vision-and-Language at the Australia Institute of Machine Learning. Australian Academy of Science awarded him a J G Russell Award in 2019. He obtained his PhD degree in 2015 and MSc degree in 2011, in Computer Science from the University of Bath, United Kingdom. His research interests are mainly in computer vision and machine learning. Currently, he is working on the vision-language problem, and he is primarily an expert in image captioning and visual question answering (VQA). He has published more than 130 papers in prestigious conferences and journals, such as TPAMI, CVPR, ICCV, ECCV. He is also the Area Chair for CVPR, ICCV and NeurIPS. More Details:  When: Tue 08 Oct 2024, at 11:00 am - 12:00 pm (Brisbane time) Speaker: A/Prof Qi Wu (University of Adelaide) Host: Prof Helen Huang Venue: 78-343, General Purpose South Zoom: https://uqz. zoom. us/j/85098152567 [Recording]"
    }, {
    "id": 44,
    "url": "/llm4graph-chao/",
    "title": "No.24-11 Graph Foundation Model in the Era of LLMs",
    "body": "2024/09/12 - Graph data structures play a crucial role in real life, effectively illustrating the complex relationships and structural dependencies between entities. In recent years, the generalization capabilities of graph models have garnered significant attention. We can’t help but wonder about the possibilities that could emerge if we fully leveraged large models to address these challenges. This is the central focus of our research: developing large language models and foundational models specifically tailored for graph data. Our strategy involves designing LLMs that can effectively encode and reason about graph structures, capturing the intricate relationships among entities. By applying the principles of language models to the domain of graph data, we aim to deepen our understanding of graph structures, ultimately achieving more powerful and scalable graph analysis to tackle real-world challenges in learning data relationships. Speaker Bio: Chao Huang is an Assistant Professor and PhD supervisor in the Department of Computer Science and the Institute of Data Science at the University of Hong Kong. His research interests encompass large language models (LLMs), graph learning, recommender systems, and spatiotemporal data mining. His research papers in these areas have garnered widespread recognition, being regarded as some of the most influential and highly cited works at conferences such as KDD 2024, WSDM 2024, WWW 2024/2023, SIGIR 2024/2023/2022, and KDD 2019. Additionally, his research has received nominations for Best Paper Awards at conferences including WWW 2023, WSDM 2022, and WWW 2019. His academic contributions have earned him the title of “Rising Star” at the WAIC World Artificial Intelligence Conference (WAIC), as well as the “2024 Frontier Science Award in Theoretical Computer Science and Information Science. ” https://sites. google. com/view/chaoh More Details:  When: Wed 16 Oct 2024, at 1:00 - 2:00 pm (GMT+10) Speaker: Prof Chao Huang (University of Hong Kong) Host: Dr Ruihong Qiu Venue: Online Zoom: https://uqz. zoom. us/j/88003098168 [Recording]"
    }, {
    "id": 45,
    "url": "/gfm-haitao/",
    "title": "No.24-08 Towards Graph Foundation Model",
    "body": "2024/09/03 - Graph Foundation Models (GFMs) is a single (neural) model that learns transferable graph representations that can generalize to any new, previously unseen graph. In this talk, we will introduce: (1) The long-term technical goal will the GFMs serve (2) The knowledge gap in the graph domain the GFMs can fill (3) The critical problem GFMs can solve. Speaker Bio: Haitao Mao is the final year PhD student in Michigan State University. His research interests lies in developing Graph Foundation Model grounded in network science and LLMs for pragmatics. He is the organizer of LOG conferenece 2023. He won the CIKM 2021 best short award and WSDM 2024 best paper honor mention award as the first author. https://haitaomao. github. io/me/ More Details:  When: Wed 11 Sept 2024, at 1:00 - 2:00 pm (GMT+10) Speaker: Haitao Mao (Michigan State University) Host: Ruihong Qiu Venue: Online Zoom: https://uqz. zoom. us/j/84584183045 [Recording]"
    }, {
    "id": 46,
    "url": "/CL-dong/",
    "title": "No.24-07 Embracing Changes in Deep Learning: Continual Learning with Augmented and Modularized Memory",
    "body": "2024/06/04 - Deep learning (DL) has been successful in many applications. However, the conventional DL approaches focus on the end results on fixed datasets/scenarios and fail to handle the dynamically raised novel requirements in the real world. Continual learning (CL) aims to train deep neural networks (DNNs) to efficiently accumulate knowledge on dynamically arriving data and task streams like humans. The main challenges include how to enable DNNs to learn on data and task streams with non-stationary distributions without catastrophic forgetting, requiring the balance between stability and plasticity. To ensure DNNs effectively retain past knowledge while accommodating future tasks, we explore CL techniques from the viewpoint of augmenting and modularizing the memorization of DNNs. We also delve into performing continual learning with or for pre-trained models. Moreover, the requirement of enabling models to detect changes presents a significant challenge in real-world DL applications. In response, part of our research focuses on relevant tasks such as out-of-distribution/anomaly detection to address these demands. Speaker Bio: Dong Gong is a Senior Lecturer and ARC DECRA Fellow (2023-2026) at the School of Computer Science and Engineering (CSE), The University of New South Wales (UNSW). He is also holding an adjunct position at the Australian Institute for Machine Learning (AIML) at The University of Adelaide. Previously, after obtaining PhD degree in Dec 2018, Dong worked as a Research Fellow at the AIML, and a Principal Researcher at the Centre for Augmented Reasoning (CAR) at the University of Adelaide. His research interests are in computer vision and machine learning. Recently, he has been focusing on learning tasks with dynamic requirements and non-ideal supervision in real-world scenarios, such as continual learning. https://donggong1. github. io/ More Details:  When: Tue 11 June 2024, at 2:30 - 3:30 pm (GMT+10) Speaker: Dr Dong Gong (The University of New South Wales) Host: Dr Yadan Luo Venue: 78-224 Zoom: https://uqz. zoom. us/j/82736212365"
    }, {
    "id": 47,
    "url": "/LMs-jain/",
    "title": "No.24-05 Efficient and Elastic Large Models",
    "body": "2024/05/15 - Generative LLMs are transforming multiple industries and have proven to be robust for multitude of use cases across industries and settings. One of the key impediments to their widespread deployment is the cost of serving and its deployability across multiple devices/settings. In this talk, we will discuss the key challenges in improving efficiency of LLM serving. We will then give an overview of multiple techniques to address the problem. In particular, we will discuss tandem transformers and HIRE, techniques to speed up decoding in LLMs. Speaker Bio: Prateek Jain is a Principal Scientist at Google Research India where he is also the director of Machine Learning and Optimization. He obtained his doctorate from UT Austin and BTech from IIT-Kanpur. He has conducted foundational research in the areas of efficient and elastic large models as well as in large-scale and non-convex optimization. Prateek regularly serves on the senior PC of top ML conferences and is on the editorial board of top ML journals including JMLR, SIMODS. He has also won multiple best paper awards including the 2020 Best Paper by IEEE Signal Processing Society. Prateek also received the Young Alumnus Award from IIT Kanpur in 2021 and the ACM India Early Career Researcher Award in 2022. More Details:  When: Fri 17 May 2024, at 3:00 - 4:30 pm (GMT+10) Speaker: Dr Prateek Jain (Google Research India) Host: Dr Mahsa Baktashmotlagh &amp; Prof Guido Zuccon Venue: 50-N201 - Hawken Engineering Building Zoom: Physical only"
    }, {
    "id": 48,
    "url": "/genrecsys-sasha/",
    "title": "No.24-06 Generative Sequential Recommendation",
    "body": "2024/05/14 - In this talk, we first introduce the Sequential Recommendation problem and draw parallels between language modelling and recommender systems. To set the stage, we also briefly cover state-of-the-art methods, such as BERT4Rec, the traditional “score-and-rank” approach for producing recommendations, and describe typical recommendation goals (accuracy, diversity, novelty, etc. ). Moving forward, then demonstrate why the “score-and-rank” approach may fail when recommendation goals include beyond-accuracy objectives, such as diversity. We show how this problem can be solved using the generative recommendations approach and how we can optimise generative recommendation models for almost any recommendation goal using reinforcement learning. We also address the challenges of generative recommendation models, such as large catalogues that can be much larger when compared to the vocabularies of language models and data sparsity.  Finally, we will discuss the role of generative Large Language Models in the future of sequential recommendation. Speaker Bio: Aleksandr Petrov is a last year PhD candidate at the University of Glasgow, specialising in the usage of Transformers for recommendation with a large catalogue. His scholarly contributions include papers in such venues as RecSys, WSDM, ECIR and ToRS. He received the Best Paper Award at RecSys 2023 and was nominated for the Best Student Paper Award at RecSys 2022.  Before his PhD, he accumulated over ten years of industry experience with big tech companies such as Amazon and Yandex, and he co-founded a recommendation startup, E-Contenta. Aleksandr also has experience lecturing in Big Data and ML courses, presenting RecSys tutorials, and invited talks. More Details:  When: Wed 22 May 2024, at 3:00 - 4:00 pm (GMT+10) Speaker: Aleksandr Petrov (University of Glasgow) Host: Dr Ruihong Qiu Venue: Online only Zoom: https://uqz. zoom. us/j/81016859627"
    }, {
    "id": 49,
    "url": "/survival-weijia/",
    "title": "No.24-04 Deep Copula-Based Survival Analysis for Dependent Censoring",
    "body": "2024/04/11 - Censoring is the central problem in survival analysis where either the time-to-event (for instance, death) or the time-to-censoring (such as loss of follow-up) is observed for each sample. The majority of existing survival analysis methods assume that survival is conditionally independent of censoring given a set of covariates, an assumption that cannot be verified since only marginal distributions are available from the data. The existence of dependent censoring and the inherent bias in current estimators has been demonstrated in various applications, accentuating the need for a more nuanced approach. However, existing methods that adjust for dependent censoring require practitioners to specify the ground truth copula. This requirement poses a significant challenge for practical applications, as model misspecification can lead to substantial bias. This talk will discuss a flexible survival analysis method that simultaneously accommodates dependent censoring and eliminates the requirement for specifying the copula. We theoretically prove the identifiability of our model under a broad family of copulas and survival distributions and empirically demonstrate that our method achieves significantly lower estimation bias when compared to existing approaches. Speaker Bio: Dr. Weijia Zhang holds the position of lecturer in the Data Science and Statistics discipline at the University of Newcastle. Prior to this role, he worked in various academic positions at the University of South Australia and Southeast University, China. His research primarily focuses on causal inference, weakly-supervised machine learning, and survival analysis. Dr. Zhang contributes to the program committee of many international conferences, including AAAI, AISTATS, ICML, IJCAI, NeurIPS, UAI, and ICLR. Additionally, he holds editorial positions with the journals Computers in Industry and Data Science &amp; Engineering. More Details:  When: Wed 24 April 2024, at 1:00 pm (GMT+10) Speaker: Dr Weijia Zhang (University of Newcastle) Host: Dr Miao Xu Venue: 49-313A Zoom: https://uqz. zoom. us/j/82441902377"
    }, {
    "id": 50,
    "url": "/openworld-jianfei/",
    "title": "No.24-03 Towards Open-World Object Segmentation and Detection",
    "body": "2024/03/14 - Segmentation and detection are two fundamental and classical tasks in computer vision. In recent years, significant attention has been devoted to the open-vocabulary object segmentation and detection, aiming to generalize beyond the limited number of classes labeled during training and segment or detect objects described by arbitrary category names at inference. Compared with conventional ones, open vocabulary object segmentation and detection largely extend the object categories. In this talk, I will introduce a few works that have been done in my group along this line, showing the trends from close-set to open-set and to open-world. Speaker Bio: Jianfei Cai is a Professor at Faculty of IT, Monash University, where he had served as the inaugural Head for the Data Science &amp; AI Department. Before that, he was Head of Visual and Interactive Computing Division and Head of Computer Communications Division in Nanyang Technological University (NTU). His major research interests include computer vision, deep learning and multimedia. He has successfully trained 30+ PhD students with three getting NTU SCSE Outstanding PhD thesis award and one getting Monash FIT Graduate Research Student Excellence Award. Many of his PhD students joined leading IT companies such as Facebook, Apple, Amazon, and Adobe or become faculty members in reputable universities. He is a co-recipient of paper awards in ACCV, ICCM, IEEE ICIP and MMSP. He serves or has served as an Associate Editor for TPAMI, IJCV, IEEE T-IP, T-MM, and T-CSVT as well as serving as Area Chair for CVPR, ICCV, ECCV, IJCAI, ACM Multimedia, ICME, ICIP and ISCAS. He was the Chair of IEEE CAS VSPC-TC during 2016-2018. He had served as the leading TPC Chair for IEEE ICME 2012 and the best paper award committee chair &amp; co-chair for IEEE T-MM 2020 &amp; 2019. He is the leading General Chair for ACM Multimedia 2024, and a Fellow of IEEE. More Details:  When: Wed 10 April 2024, at 1:00 pm (GMT+10) Speaker: Prof Jianfei Cai (Monash University) Host: Dr Yadan Luo Venue: 49-313A Zoom: https://uqz. zoom. us/j/84587453556Zoom Recording: Not available yet "
    }, {
    "id": 51,
    "url": "/amazon-garcia/",
    "title": "No.24-02 Building AI/ML & Gen AI responsibly on AWS",
    "body": "2024/03/07 - This presentation explores best practices for building AI/ML and generative AI (Gen AI) models responsibly on AWS. We’ll explore real use cases where these technologies are driving value for organizations in Education and Research. We’ll cover AWS’s framework for responsible AI, focusing on transparency, fairness, privacy, and accountability. Attendees will learn about tools and services like Amazon SageMaker and Amazon Bedrock that support ethical AI development. Speaker Bio: Nieves is the AI/ML Specialist lead for Public Sector in Amazon Web Services (AWS). She drives generative AI, artificial intelligence and machine learning strategies from inception to scale, helping organizations across Education, Research, Healthcare and Government, among others, to transform their business leveraging AI/ML. Previous to AWS, Nieves was ecosystem director and new products introduction director in different start-ups and large organisations in Europe, having the privilege of contributing to several standards and patents along the way. For the last 15 years, her work has been focused on driving innovation and bringing new products to market globally, particularly in the Customer Experience, Internet of Things and AI/ML space. Currently based in Australia, she’s an active mentor and advocate of diversity and inclusion in STEM and AI, collaborating with various Universities and government entities in Asia Pacific and Europe. She’s a lecturer &amp; Industry expert for the Machine Learning Institute in Spain. More Details:  When: Wed 13 March 2024, at 1:00 pm (GMT+10) Speaker: Nieves Garcia (Amazon) Host: Prof Shazia Sadiq Venue: 49-313A Zoom: https://uqz. zoom. us/j/83192039778Zoom Recording: Not available yet "
    }, {
    "id": 52,
    "url": "/chen-gao/",
    "title": "No.24-01 Filter Bubble in Recommender System: Diversity and Beyond",
    "body": "2024/03/05 - Recommender system has reshaped how we access information in today’s world, make relevant content more accessible to everyone. However, it has also resulted in some negative side-effects, such as the filter bubble. The filter bubble refers to the phenenomon that the content the user is exposed to becomes more and more homogeneous and limited, causing increased bias and polarization. In this talk, we first discuss one of the most important aspects of the filter bubble, recommendation diversity, particularly diversified recommendation models and full-stage large-scale online experiments on short-video platforms. Then, we will present the quantitive analysis of the filter bubble on one of the largest short-video platforms, introducing a novel concept: deep filter bubble. Last, we will present a general and easy-to-use framework of controllable recommender system to break out of the filter bubble. Speaker Bio: Chen Gao is now a Research Assistant Professor of BNRist, Tsinghua University. He obtained his Ph. D. Degree and Bachelor’s Degree from the Department of Electronic Engineering, Tsinghua University in 2021 and 2016, respectively. His research primarily focuses on data mining and information retrieval, with over 60 papers in top-tier venues (50+ CCF-A), including SIGIR, WWW, KDD, TKDE, TOIS, ICDE, ICLR, NeurIPS, MM, UbiComp, CSCW, NDSS, etc. , attracting over 2500 citations. His work on GNN-based bundle recommendation received the Best Short Paper Honorable Mention Award in SIGIR 2020. He serves as the PC member for conferences such as WWW, KDD, SIGIR, WSDM, CIKM, NeurIPS, ICLR, ICML, MM, RecSys, AAAI, IJCAI, AISTATS, ECML-PKDD, etc. , and the regular reviewer for journals including IEEE TKDE, ACM TOIS, etc. He was selected as one of Top 100 Chinese Rising Stars in Artificial Intelligence by Baidu Scholar in 2021. He was also at the finalist of 2021 China Computer Federation (CCF) Outstanding Doctoral Dissertation Award. He has also organized multiple tutorials in top-tier conferences including WWW, KDD, WSDM, IJCAI, etc. More Details:  When: Thu 07 March 2024, at 1:00 pm (GMT+10) Speaker: Prof Chen Gao (Tsinghua University) Host: Dr Ruihong Qiu Venue: Virtual Zoom: https://uqz. zoom. us/j/85342951204Zoom Recording: Not available yet "
    }, {
    "id": 53,
    "url": "/welcome-to-uqds/",
    "title": "Welcome to UQ Data Science Seminar Series!",
    "body": "2023/01/25 - This UQ Data Science seminar series aims to bring together students and senior researchers to discuss about their research, intending to have a diverse set of talks and speakers on topics realted to machine learning, computer vision, data mining, information retrieval, etc. UQ Data Science Team is a leading research group in Australia. The DS group’s research standing ensured UQ received ERA ranking of 5 in 0806 Information Systems in the 2012-2018 collections. DS has been working in the area of large-scale databases and information systems, involving a wide range of data types, especially multimedia data which is the type of data under consideration of this project. DS has a growing portfolio of datasets and data-intensive computing infrastructure capable to support terabyte-level highperformance storage and processing. The established Big Data lab by the DS group will support the work envisaged in this project. DS members have consistently attracted research funding from ARC and the industry. The group has a successful track record of publishing in top conferences and journals with over 200 publications in the last 5 years. DS members have advised nearly 70 RHD students in the last 5 years. DS contributes significantly to the national and international research community in various leadership roles including leading the ARC Training Centre for Information Resilience, hosting CIKM’21, ACM Multimedia’15, ICDE’13. Subscribe to us (Enter your email address below) and get notified when new talk is available! "
    }, {
    "id": 54,
    "url": "/process-mining/",
    "title": "No.23-22 Process Mining: Opportunities and Challenges",
    "body": "2022/12/11 - In recent years, significant advances in extended reality (XR) and AI have offered novel, promising ways to visualise and eventually better understand large and complex data. Immersion augments our ability to visualise data beyond the 2D screen, offers more display space and more direct interaction with the data. AI can perform, for example, advanced tasks such as exploratory data analysis, or sophisticated classification and segmentation of data. How do these two technologies merge to better inform human-in-the-loop approaches to support data-related tasks? In this talk I will first briefly introduce my previous work in Immersive Analytics, which includes the visualisation of abstract and 3D data in multiple XR scenarios. I will then present my recent work on immersive and interactive 3D point cloud classification through a dialogue with a machine learning model in Virtual Reality, to support a classification task. Speaker Bio: Prof Wynn conducts research in the areas of business process management, process mining, and data quality, having completed her PhD in workflow management, and process automation in 2007. She is a Vice-Chair and one of the steering committee members of the IEEE Taskforce on Process Mining. As IEEE Working Group Chair, she serves to re-standardise the eXtensible Event Stream (XES), an input data format for process mining. Moe has steered large collaborative research themes in Robotic Process Automation, Digital Health Analytics, Privacy-preserving process mining, and Data Quality. She has over fifteen years of experience in conducting applied research across multiple Australian sectors engaging with logistics, healthcare, insurance, utility, education, government, mining, and agri-food supply chains to pinpoint inefficiencies and derive data-driven improvements. Her research expertise is recognised in her 100+ refereed papers, including 35 journal articles (Google h-index:38; citations:6600+), and a Scopus h-index of 27, as of 30 July 2023. As an international BPM researcher and educator, Prof Wynn has served as a co-chair and committee member for international conferences, grant assessor for the ARC research council, PhD thesis examiner, and a reviewer of international journals. She was a program committee chair of the 2nd International Conference on Process Mining 2020 and the International Conference on Business Process Management 2021. She was a co-editor of a special Issue on Robotic Process Automation (RPA) in the Computers in Industry Journal (2021) and is a co-editor of a special Issue on Managing the Dynamics of Business Processes in the Business &amp; Information Systems Engineering journal (2023). She is a member of the Australian Research Council College of Experts (2023 – 2025). More Details:  When: Wed 18 Sep 2023, at 1:00 pm (GMT+10) Speaker: Dr Maxime Cordeil (University of Queensland) Host: Dr Helen Huang Venue: Building 14-115 Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: Recording - UQ Login Needed "
    }, {
    "id": 55,
    "url": "/immersive-data-visualisation/",
    "title": "No.23-20 Immersive Data Visualisation and Interactive AI",
    "body": "2022/12/11 - In recent years, significant advances in extended reality (XR) and AI have offered novel, promising ways to visualise and eventually better understand large and complex data. Immersion augments our ability to visualise data beyond the 2D screen, offers more display space and more direct interaction with the data. AI can perform, for example, advanced tasks such as exploratory data analysis, or sophisticated classification and segmentation of data. How do these two technologies merge to better inform human-in-the-loop approaches to support data-related tasks? In this talk I will first briefly introduce my previous work in Immersive Analytics, which includes the visualisation of abstract and 3D data in multiple XR scenarios. I will then present my recent work on immersive and interactive 3D point cloud classification through a dialogue with a machine learning model in Virtual Reality, to support a classification task. Speaker Bio: Dr. Maxime Cordeil is a Senior Lecturer in the Electric Engineering and Computer Science (EECS) School at UQ. Before joining, he was a Postdoc and then a Lecturer in the Immersive Analytics research group at Monash University. He obtained his PhD from the University of Toulouse and the Higher French Institute of Aeronautics and Space, France. In 2021 and 2023, Dr Cordeil has been recognised as Field Leader in Computer Graphics, for his work on Visualisation, by The Australian Research Magazine yearly ranking. He has authored over 60 publications in major conferences and journals in the domains of data visualisation and analytics, human-computer interaction and XR, which include IEEE VIS/TVCG, ACM CHI/UIST, ISMAR and IEEE VR. More Details:  When: Wed 20 Sep 2023, at 1:00 pm (GMT+10) Speaker: Dr Maxime Cordeil (University of Queensland) Host: Dr Gianluca Demartini Venue: Building 14-115 Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: "
    }, {
    "id": 56,
    "url": "/dynamic-graph/",
    "title": "No.23-19 Graph Neural Networks for Large Dynamic Graphs",
    "body": "2022/12/11 - In real-world applications such as social networks, financial transactions, and recommender systems, graph-structured data is frequently dynamic, with the nodes and edges of the graph constantly developing dynamically. While Graph Neural Networks (GNNs) have risen as formidable tools for modeling graph-structured data, their adaptation to dynamic graphs introduces distinct challenges. First, any alteration in the graph necessitates a complete relearning of the graph representation matrix, which is expensive and time-consuming. Secondly, even as existing dynamic GNNs are optimized for learning temporal information, they encounter difficulties in scaling to large, evolving graphs. To address these problems, my research focuses on improving the scalability and expressiveness of dynamic graph neural networks. In my talk, I will first highlight the significance of efficiently computing the graph representation matrix, and then introduce our InstantGNN model. This model incrementally calculates the graph propagation matrix in dynamic graphs, enhancing computational efficiency while ensuring the learning capabilities of GNNs. Next, I will describe the Decoupled Dynamic Graph Neural Network framework. This method supports large dynamic graph learning, where generalized dynamic propagation can effectively support efficient computation on various types of dynamic graphs. Together, these directions help pave a path forward for large dynamic graphs learning. Speaker Bio: Yanping Zheng is now working as a visiting scientist at DATA61, CSIRO. She is currently a third-year Ph. D. candidate at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Professor Zhewei Wei. Her research focuses on graph learning algorithms. She is particularly interested in efficient algorithms on Graph Neural Networks, Dynamic Graph Representation Learning. More Details:  When: Wed 13 Sep 2023, at 1:00 pm (GMT+10) Speaker: Yanping Zheng (DATA61-CSIRO) Host: Dr Sen Wang Venue: Building 14-115 Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: "
    }, {
    "id": 57,
    "url": "/generalized-ood/",
    "title": "No.23-16 Generalized Out-of-distribution Detection: Theory and Algorithm",
    "body": "2022/12/11 - Out-of-distribution (OOD) detection is vital for ensuring the safety and reliability of artificial intelligence systems. It represents a novel and trending area in machine learning and artificial intelligence. The concept of OOD detection was first proposed in 2017 and has since shown significant potential in enabling the reliable deployment of machine learning models in real-world applications, including medical safety and autonomous driving system. Over the past few years, a rich line of algorithms has been developed to address the generalized OOD detection problem empirically. In this talk, we will present the latest advancements in OOD detection theory, and OOD detection algorithms. Speaker Bio: Dr Zhen Fang is currently a Lecturer (Research) at the Australian Artificial Intelligence Institute, University of Technology Sydney (UTS), working with Prof. Jie Lu.  He received his Ph. D degree in artificial intelligence from UTS (2018-2022), supervised by Prof. Jie Lu. His research interests include transfer learning, statistical learning theory and out-of-distribution learning, and his works have been published in top AI journals and AI conferences e. g. , NeurIPS, ICML and IEEE-TPAMI.  Recently, Zhen also received the Outstanding Paper Award in NeurIPS 2022 for his work related to out-of-distribution detection theory. More Details:  When: Wed 23 Aug 2023, at 1:00 pm (GMT+10) Speaker: Dr Zhen Fang (University of Technology Sydney) Host: Dr Yadan Luo Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: "
    }, {
    "id": 58,
    "url": "/how-to-detect-ood/",
    "title": "No.23-15 How to Detect Out-of-Distribution Data in the Wild? Challenges, Research Progress and Path Forward",
    "body": "2022/12/10 - When deploying machine learning models in the open and non-stationary world, their reliability is often challenged by the presence of out-of-distribution (OOD) samples. Since data shifts happen prevalently in the real world, identifying OOD inputs has become an important problem in machine learning. In this talk, I will discuss challenges, research progress, and opportunities in OOD detection. Our work is motivated by the insufficiency of existing learning objective such as ERM — which focuses on minimizing error only on the in-distribution (ID) data, but do not explicitly account for the uncertainty that arises outside ID data. To mitigate the fundamental limitation, I will introduce a new algorithmic framework, which jointly optimizes for both accurate classification of ID samples, and reliable detection of OOD data. The learning framework integrates distributional uncertainty as a first-class construct in the learning process, thus enabling both accuracy and safety guarantees. Speaker Bio: Sharon Yixuan Li is an Assistant Professor in the Department of Computer Sciences at the University of Wisconsin-Madison. She received a Ph. D. from Cornell University in 2017, advised by John E. Hopcroft. Subsequently, she was a postdoctoral scholar in the Computer Science department at Stanford University. Her research focuses on the algorithmic and theoretical foundations of learning in open worlds. She has served as Area Chair for ICLR, NeurIPS, ICML, and Program Chair for Workshop on Uncertainty and Robustness in Deep Learning. She is the recipient of the AFOSR Young Investigator Program (YIP) award, the NSF CAREER award, Forbes30Under30 in Science, and multiple faculty research awards from Google, Meta, and Amazon. Her works received a NeurIPS Outstanding Paper Award, and an ICLR Outstanding Paper Award Honorable Mention in 2022. More Details:  When: Wed 16 Aug 2023, at 10:00 am (GMT+10) Speaker: Dr Sharon Yixuan Li (University of Wisconsin-Madison) Host: Dr Yadan Luo Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: Zoom Link "
    }, {
    "id": 59,
    "url": "/task-aware-retrieval/",
    "title": "No.23-13 Task-aware Retrieval with Instructions ",
    "body": "2022/12/10 - We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose task-aware retrieval system using multi-task instruction tuning, which can follow human-written instructions to find the best documents for a given query. We introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, BERRI, and present TART, a multi-task retrieval system trained on BERRI with instructions. TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X^2-Retrieval to better reflect real-world scenarios, where diverse domains and tasks are pooled and a system needs to find documents aligning users’ intents. In this setup, TART significantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions. Speaker Bio: Akari Asai is a Ph. D. student in the Paul G. Allen School of Computer Science &amp; Engineering at the University of Washington, advised by Prof. Hannaneh Hajishirzi. Her research lies in natural language processing and machine learning. Her recent research focuses on question answering, multilingual NLP, and NLP efficiency. She received the IBM Fellowship in 2022 and the Nakajima Foundation Fellowship in 2019. Prior to UW, she obtained a B. E. degree in Electrical Engineering and Computer Science from the University of Tokyo. More Details:  When: Wed 02 Aug 2023, at 1:00 pm (GMT+10) Speaker: Akari Asai (University of Washington) Host: Prof Guido Zuccon Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: Zoom link "
    }, {
    "id": 60,
    "url": "/session-recommendation/",
    "title": "No.23-12 An Introduction to Sequential/Session-based Recommendation ",
    "body": "2022/12/10 - In recent years, sequential/session-based recommendations have emerged as a new recommendation paradigm to well model users’ dynamic and short-term preferences for more accurate and timely recommendations. They have been employed in various application domains, including e-Commerce, video platforms/apps, news websites/apps, etc. and have achieved great success. In this talk, I will briefly share some of our work in this vibrant research area. First, I will introduce the background, and research problem of sequential/session-based recommendation. Then, I will introduce three specific sub-areas in this area (i. e. , next-product recommendation, next-basket recommendation, next-news recommendation) via sharing some of our specific work in each of them, followed by an overview of some applications of sequential/session-based recommendations. Finally, I will conclude this talk by sharing some future directions. Speaker Bio: Shoujin Wang is a Lecturer in Data Science at University of Technology Sydney. Shoujin obtained his PhD in data science from University of Technology Sydney in 2019. His main research interests include data mining, machine learning, recommender systems and fake news mitigation.  He has published more than 50 research papers in these areas, most of which were published at premier data science and AI conferences or journals, like The WebConf, SIGIR, AAAI, IJCAI, TKDE and ACM CSUR. Shoujin has generally served as a PC member or a senior PC member at over 10 premier international data science conferences including KDD, AAAI, IJCAI, WSDM, CIKM and a reviewer for more than 10 prestigious journals including Machine Learning, IEEE TKDE, ACM TOIS, etc. Shoujin is the recipient of 2021 DAAD AINet Fellowship, 2022 Club Melbourne Fellowship and 2022 DSAA Next-generation Data Scientist Award. More Details:  When: Wed 17 May 2023, at 1:00 pm (GMT+10) Speaker: Dr Shoujin Wang (University of Technology Sydney) Host: Dr Rocky Chen Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: "
    }, {
    "id": 61,
    "url": "/weights-biases/",
    "title": "No.23-11 Building Experiment Tracking at Scale with Weights & Biases",
    "body": "2022/12/09 - Building experiments doesn’t just end once the model is deployed. Teams need to monitor their models in production and use their findings to iterate further. Especially when dealing with tens to hundreds of models you need to monitor and automate at scale! How do you track your experiments? Have you ever found it hard to reproduce them and is sharing results with your colleagues and managers a headache?Join us on May 10, at 1 PM AEST to learn all about experiment tracking at scale with Pachyderm and Weights &amp; Biases. W&amp;B introduction and product walkthrough (talk will touch on: experiment tracking, W&amp;B Tables, Sweeps, Artifacts, Dashboards/Reports, Integrations), followed by a Colab classification competition in Kaggle with swag for top submissions Speaker Bio: Andrea is a Machine Learning engineer who has worked extensively with large-scale graphs. She has a degree in Information Retrieval from the University of Michigan School of Information and has been working in the industry for over a decade. Andrea is passionate about data-driven problem solving and enjoys working on NLP and IR challenges across varied domains: health data, networks/linked data, and more. More Details:  When: Wed 10 May 2023, at 1:00 pm (GMT+10) Speaker: Andrea Parker, Growth ML Engineer (Weights &amp; Biases) Host: Dr Yadan Luo Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: "
    }, {
    "id": 62,
    "url": "/SEINE-retrieval/",
    "title": "No.23-10 SEINE: SEgment-based Indexing for NEural Information Retrieval",
    "body": "2022/12/08 - Many early neural Information Retrieval (NeurIR) methods are re-rankers that rely on a traditional first-stage retriever due to expensive query time computations. Recently, representation-based retrievers have gained much attention, which learn query representation and document representation separately, making it possibleto pre-compute document representations offline and reduce the workload at query time. Both dense and sparse representation-based retrievers have been explored. However, these methods focus on finding the representation that best represents a text (aka metric learning), and the actual retrieval function that is responsiblefor similarity matching between query and document is kept at a minimum by using dot product. One drawback is that, unlike a traditional term-level inverted index, the index formed by these embeddings cannot be easily re-used by another retrieval method. Another drawback is that keeping the interaction at a minimum hurtsretrieval effectiveness. On the contrary, interaction-based retrievers are known for their better retrieval effectiveness. In this work, we propose a novel SEgment-based Neural Indexing method, SEINE, which provides a general indexing framework that can flexibly support a variety of interaction-based neural retrievalmethods. We emphasize a careful decomposition of common components in existing neural retrieval methods and propose to usea segment-level inverted index to store the atomic query-document interaction values. Experiments on LETOR MQ2007 and MQ2008datasets show that our indexing method can accelerate multiple neural retrieval methods up to 28 times faster without sacrificingmuch effectiveness. Short Bio: Dr. Grace Hui Yang is Associate Professor in Computer Science at Georgetown University, Washington D. C. Dr. Yang is leading the InfoSense (Information Retrieval and Sense-Making) group at Georgetown University. Dr. Yang obtained her Ph. D. from Carnegie Mellon University in 2011. Her current research interests include deep reinforcement learning, interactive agents, search engines, and privacy-preserving information retrieval. Prior to this, she conducted research on question answering, automatic ontology construction, near-duplicate detection, multimedia information retrieval, and opinion and sentiment detection. Dr. Yang’s research has been supported by the Defense Advanced Research Projects Agency (DARPA) and the National Science Foundation (NSF). Dr. Yang led the effort for the Text Retrieval Conference (TREC) Dynamic Domain Tracks from 2015 to 2017 and SIGIR privacy-preserving information retrieval workshops from 2014 to 2016 and co-organized the SIGIR Deep Reinforcement Learning Workshops since 2020. Dr. Yang is associate editor for ACM Transactions on Information Systems and served on the editorial board of Information Retrieval Journal from 2014 to 2017. She has actively served as an organizing or program committee member in many top-tier international conferences such as SIGIR, ECIR, ACL, AAAI, ICTIR, CIKM, WSDM, and WWW, and the general co-chair of SIGIR 2024. She is a recipient of the NSF Faculty Early Career Development Program (CAREER) Award and a co-author of the 2016 book “Dynamic Information Retrieval Modeling. ” More Details:  When: Wed 3 May 2023, at 10:00 am (GMT+10) Speaker: Dr Grace Hui Yang (Georgetown University) Host: Prof Helen Huang Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: "
    }, {
    "id": 63,
    "url": "/mobility-digital-twin/",
    "title": "No.23-09 Mobility Digital Twin for Connected and Automated Vehicles",
    "body": "2022/12/07 - A Digital Twin is a digital replica of a living or nonliving physical entity, and this emerging technology attracted extensive attention from different industries during the past decade. In this talk, a Mobility Digital Twin (MDT) framework is introduced, which is defined as an Artificial Intelligence (AI)-based data-driven cloud-edge-device framework for mobility services. This MDT consists of three building blocks in the physical space (namely Human, Vehicle, and Traffic), and their associated Digital Twins in the digital space. An example cloud-edge architecture is built with Amazon Web Services (AWS) to accommodate the proposed MDT framework and to fulfill its digital functionalities of storage, modeling, learning, simulation, and prediction. The effectiveness of the MDT framework is presented by the case studies of multiple connected and automated vehicle (CAV) applications, benefiting our transportation systems regarding safety, efficiency, and environmental sustainability. Short Bio: Dr. Ziran Wang is currently an Assistant Professor in the Lyles School of Civil Engineering at Purdue University, where he directs the Purdue Digital Twin Lab. Previously, he worked as a Principal Researcher at Toyota Motor North America R&amp;D in Silicon Valley, leading the Digital Twin roadmap at Toyota. Dr. Wang is serving as associate editor of IEEE Internet of Things Journal, IEEE Transactions on Intelligent Vehicles, and two other journals. He is also founding chair of IEEE Technical Committee on “Internet of Things in Intelligent Transportation Systems”. Dr. Wang is an author of 40+ peer-review publications and 50+ patent applications. More Details:  When: Wed 26 April 2023, at 1:00 pm (GMT+10) Speaker: Dr Ziran Wang (Purdue University) Host: Dr Yadan Luo Zoom: https://uqz. zoom. us/j/82896549343Zoom Link: https://uqz. zoom. us/j/82896549343 Recording: https://uqz. zoom. us/rec/share/1U2Ggz11k22k1GAd9GAEYf9lslorHugKYvgiVcHxZYtHiAe-i9UYe2cdWZRI2A80. HCDhWYFU0J90B7PC?startTime=1682477806000 "
    }, {
    "id": 64,
    "url": "/human-quotient-ai-systems/",
    "title": "No.23-05 A magic ingredient, a secret spice, a special blend, for it can all be nice!' The Human Quotient for Better AI Systems",
    "body": "2022/12/06 - The unprecedented rise in the adoption of artificial intelligence techniques and automation in many contexts is concomitant with the shortcomings of such technology concerning robustness, interpretability, usability, trustworthiness, and explainability. Crowd computing offers a viable means to leverage human intelligence at scale for data creation, enrichment, and interpretation, demonstrating a great potential to improve the performance of AI systems and increase AI adoption in general. Can we help pave a future where humans can benefit by working seamlessly with AI? What can we do to facilitate appropriate trust and reliance of people on AI systems? This talk will discuss the intriguing and pertinent role of human input in propelling better AI technology in the quickly evolving age of generative models. Short Bio: Ujwal Gadiraju is an Assistant Professor at the Web Information Systems (WIS) group of the Software Technology department at Delft University of Technology, the Netherlands. He is a Director of the Delft AI ‘Design@Scale’ Lab, which focuses on Human-AI collaboration to design solutions for complex social problems. Ujwal leads a research line on Crowd Computing and Human-Centered AI at the WIS group. He is an ACM Distinguished Speaker. Before joining the WIS group, Ujwal worked at the L3S Research Center as a Postdoctoral researcher between 2017-2020. He received a PhD (Dr. rer. nat. ) in Computer Science from the Leibniz University of Hannover, Germany, in 2017 and an MSc. Computer Science degree from TU Delft, the Netherlands, in 2012. His research interests lie at the intersection of Artificial Intelligence (AI) and Human-Computer Interaction (HCI). He has published over 135 peer-reviewed articles, including at premier venues such as ACM CHI, ACM CSCW, ACM TOCHI, AAAI HCOMP, ACM TheWebConf, ACM SIGIR, ACM UBICOMP, ACM CIKM, ACM WSDM, ACM HT, ACM UMAP, among others. His work has been recognized with various honors, including best paper awards at top-tier HCI and AI conferences. Ujwal’s prior work in Crowd Computing has explored methods to improve the effectiveness of the crowdsourcing paradigm, running large-scale human-centered experiments to understand the interaction between humans and machines and the societal impact of algorithmic decision-making. Ujwal’s goal is to create novel methods, interfaces, systems, and tools to overcome existing challenges on our path towards building better AI systems and facilitating better reliance of humans on AI systems. For more information about his research and other activities, please visit https://ujwalgadiraju. com. More Details:  When: Wed 22 March 2023, at 1:00 pm (GMT+10) Speaker: Dr Ujwal Gadiraju (Delft University of Technology) Host: A/ Professor Gianluca Demartini Venue: Building 46, Room 914 Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: "
    }, {
    "id": 65,
    "url": "/multidomain-fewshot/",
    "title": "No.23-04 Multi-Domain Few-Shot Image Classification ",
    "body": "2022/12/05 - Most existing few-shot classification methods only consider generalization on one dataset (i. e. , single-domain), failing to transfer across various seen and unseen domains. In this talk, I will introduce the more realistic multi-domain few-shot classification problem to investigate the cross-domain generalization. Specifically, I will elaborate our ICCV 2021 work which designed a parameter-efficient multi-mode modulator (tri-M) to solve the above problem. First, the modulator is designed to maintain multiple modulation parameters (one for each domain) in a single network, thus achieving single-network multi-domain representation. Given a particular domain, domain-aware features can be efficiently generated with the well-devised separative selection and cooperative query modules. Second, we further divide the modulation parameters into the domain-specific set and the domain-cooperative set to explore the intra-domain information and inter-domain correlations, respectively. We demonstrate that the proposed multi-mode modulator achieves state-of-the-art results on the challenging META-DATASET benchmark, especially for unseen test domains. Short Bio: Dr. Yanbin Liu is currently a Research Fellow in the School of Computing, Australian National University. His research interest involves few-shot learning, deep declarative networks, and spatial-temporal modeling. He has obtained 900+ Google citations, among which the ICLR 19 paper set up a new transductive few-shot benchmark and attracted 600+ followup works. He is the reviewer of major computer vision and machine learning conferences and journals, and received the outstanding reviewer award in CVPR 202 More Details:  When: Wed 15 March 2023, at 1:00 pm (GMT+10) Speaker: Dr Yanbin Liu (Australian National University) Host: Dr Xin Yu Zoom: https://uqz. zoom. us/j/82896549343"
    }, {
    "id": 66,
    "url": "/efficient-event-processing/",
    "title": "No.23-0301 Efficient Distributed Complex Event Processing",
    "body": "2021/12/04 - Complex event processing emerged as a computational paradigm to detect patterns in event streams based on the continuous evaluation of event queries. Once such queries are evaluated in a network of event sources, efficient query evaluation may be achieved through the distributed evaluation of queries. In this talk, we present some of our recent results on achieving such distribution with the model of MuSE graphs as well as optimizations that rely on push-pull-communication. Short Bio: Matthias Weidlich is a full professor at the Department of Computer Science at Humboldt-Universität zu Berlin (HU Berlin), Germany, where he holds the Chair on Databases and Information Systems. Before joining HU Berlin, he held positions at Imperial College London and at the Technion - Israel Institute of Technology. He holds a PhD in Computer Science from the Hasso-Plattner-Institute, University of Potsdam. His research focuses on data-driven process analysis, event stream processing, and exploratory data analysis. He serves as Co-Editor in Chief for the Information Systems journal and is a member of the steering committee of the ACM DEBS conference series. More Details:  When: Thurs 9 March 2023, at 1:00 pm (GMT+10) Speaker: Dr Matthias Weidlich (Humboldt-Universität zu Berlin (HU)) Host: Prof Shazia Sadiq Venue: Building 46, Room 914 Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: Zoom Recording Link "
    }, {
    "id": 67,
    "url": "/score-based-diffusiony/",
    "title": "No.23-03 Score based Diffusion Models and Their Applications ",
    "body": "2021/12/03 - Generative models show great potential in generating new samples, which have been extensively investigated in 2D/3D vision tasks. Among them, the adversarial training based models, e. g. Generative Adversarial Nets (GAN), are proven more effective in sample quality, compared with those likelihood based models, e. g. Variational Auto-encoders (VAE), Normalizing Flow (NF), etc. However, GANs show limitation in density modelling or stable -training, making it unsuitable for those likelihood-based tasks, e. g. out-of-distribution detection. Recently, score-based diffusion models are studied, and related research explains its superiority in both sample quality and density modelling. In this task, we will explain the basic idea of score-based diffusion models, and explore its potential in 2D/3D vision tasks. Short Bio: Jing Zhang is currently a Lecturer with School of Computing, the Australian National University (Canberra, Australia). Her main research interests are generative models, uncertainty estimation, weakly supervised learning. She won the Best Student Paper Prize at DICTA 2017, the Best Deep/Machine Learning Paper Prize at APSIPA ASC 2017 and the Best Paper Award Nominee at IEEE CVPR 2020. More Details:  When: Wed 8 March 2023, at 1:00 pm (GMT+10) Speaker: Dr Jing Zhang (Australian National University) Host: Dr Xin Yu Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: Zoom Recording Link "
    }, {
    "id": 68,
    "url": "/testing-uniformity/",
    "title": "No.23-02 Seven Algorithms for the Same Task (Testing Uniformity)",
    "body": "2021/12/02 - Suppose you get a set of (independent) data points in some discrete but huge domain {1,2,…,k}, and want to determine if this data is uniformly distributed. This is a basic and fundamental problem in Statistics, and has applications in computer science, not all made up: from testing the mixing time of a random walk, to detecting malicious changes in a data stream, to selecting a good algorithm depending on the input distribution. The goal, of course, is to perform this task efficiently, both time- (time complexity) and data-wise (sample complexity). In this talk, I will survey and discuss seven algorithms for uniformity testing, and explain some of their (dis)advantages. Short Bio: Clément Canonne is a Lecturer in the School of Computer Science of the University of Sydney, and an ARC DECRA fellow; he obtained his Ph. D. in 2017 from Columbia University, before joining Stanford as a Motwani Postdoctoral Fellow, then IBM Research as a Goldstine Postdoctoral Fellow. His main areas of research are distribution testing (and, broadly speaking, property testing) and learning theory; focusing, in particular, on understanding the computational aspects of learning and statistical inference subject to various resource or information constraints. More Details:  When: Wed 1 March 2023, at 1:00 pm (GMT+10) Speaker: Dr Clément Canonne (University of Sydney) Host: Dr Yadan Luo Zoom: https://uqz. zoom. us/j/82896549343Zoom Recording: "
    }, {
    "id": 69,
    "url": "/escaping-echo/",
    "title": "No.23-01 Escaping the Echo Chamber: The Quest for Normative News Recommender Systems",
    "body": "2021/12/01 - Recommender systems and social networks are often faulted to be the cause for creating Echo Chambers – environments where people mostly encounter news that match their previous choices or those that are popular among similar users, resulting in their isolation inside familiar but insulated information silos.  Echo chambers, in turn, have been attributed to be one cause for the polarization of society, which leads to the increased difficulty to promote tolerance, build consensus, and forge compromises. To escape these echo chambers, we propose to change the focus of recommender systems from optimizing prediction accuracy only to considering measures for social cohesion. This proposition raises questions in three spheres: In the technical sphere, we need to investigate how to build “socially considerate” recommender systems. To that end, we develop a novel recommendation framework with the goal of improving information diversity using a modified random walk exploration of the user-item graph. In the social sphere, we need to investigate if the adapted recommender systems have the desired effect. To that end, we present an empirical pilot study that exposed users to various sets (some diverse) of news with surprising results. Finally, in the normative sphere, these studies raise the question what kind of diversity is desirable for the functioning of democracy. Reflecting the consequences of these findings for our discipline, this talk highlights that computer science needs to increasingly engage with both the social and normative challenges of our work, possibly producing a new understanding of our discipline. It proposes similar consequences for other disciplines in that they increasingly need to embrace all three spheres. Short Bio: Abraham Bernstein, Ph. D. , is a Full Professor of Informatics at the University of Zurich (UZH), Switzerland. He received a Diploma in Computer Science from ETH Zurich and a Ph. D. in Management with a concentration in Information Technologies from the Sloan School of Management at MIT. His research focuses on various aspects of the semantic web, recommender systems, AI/data mining/machine learning, crowd computing, and collective intelligence. His work draws on both technical (computer science, artificial intelligence) and social science (organizational psychology/sociology/economics) foundations. He is also a founding Director of the University of Zurich’s Digital Society Initiative (DSI) — an interdisciplinary research and teaching initiative with more than 180 faculty members ranging from divinity to veterinary medicine investigating all aspects of the interplay between society and the digitalization – and President of the Steering Committee of the Swiss National Science Foundation’s Research Priority Program 77 on the Digital Transformation. He served as a member of the Council of Europe’s Committee of experts on Human Rights Dimensions of automated data processing and different forms of artificial intelligence (MSI-AUT) as well as on the editorial boards of various top journals, including as a co-Editor in Chief at the Journal of Web Semantics or Associate Editor at the ACM Transactions on Internet Technologies and ACM Transactions on Interactive Intelligent Systems. He was elected as an individual member of the Swiss Academy of Technical Sciences in 2019. More Details:  When: Wed 22 Feb 2023, at 1:00 pm (GMT+10) Speaker: Prof Abraham Bernstein (University of Zurich) Host: A/ Professor Gianluca Demartini Zoom: https://uqz. zoom. us/j/82896549343 Venue: Room 213 in Richards Building (5)The informal afternoon tea meet and greet introducing our UZH colleagues will follow Prof Avi Bernstein’s talk at the same location, from 2-3pm, sponsored by the Swiss Embassy. Colleagues who are planning to attend the talk in person are welcome to stay and join us for the UZH afternoon tea meet and greet. Zoom Link: https://uqz. zoom. us/j/82896549343 "
    }, {
    "id": 70,
    "url": "/QA-taxonomy/",
    "title": "A Non-Factoid Question-Answering Taxonomy",
    "body": "2021/11/09 - Non-factoid question answering (NFQA) is a challenging and under-researched task that requires constructing long-form answers, such as explanations or opinions, to open-ended non-factoid questions - NFQs. There is still little understanding of the categories of NFQs that people tend to ask, what form of answers they expect to see in return, and what the key research challenges of each category are. This work presents the first comprehensive taxonomy of NFQ categories and the expected structure of answers. The taxonomy was constructed with a transparent methodology and extensively evaluated via crowdsourcing. The most challenging categories were identified through an editorial user study. We also release a dataset of categorised NFQs and a question category classifier. Finally, we conduct a quantitative analysis of the distribution of question categories using major NFQA datasets, showing that the NFQ categories that are the most challenging for current NFQA systems are poorly represented in these datasets. This imbalance may lead to insufficient system performance for challenging categories. The new taxonomy and the category classifier will aid research in the area, helping to create more balanced benchmarks and to focus models on addressing specific categories. Short Bio: Valeriia Baranova-Bolotova, a third-year PhD candidate in Information Retrieval at RMIT University (supervised by Mark Sanderson, Falk Scholer, and Bruce Croft). Former head of the NLP research and development department at Tinkoff. Her main research interests include natural language processing, information retrieval, and machine learning. The main focus of her PhD is non-factoid multi-document question answering. She has been the first author and a co-author of several papers published in ACL, CIKM, CHIIR, and SIGIR, and the recipient of the best paper awards for her full papers in CIKM 2020 and SIGIR 2022.  More Details:  When: Wed 9 Nov 2022, at 1:00 pm (GMT+10) Speaker: Ms Valeria Baranova-Bolotova Host: A/Prof Gianluca Demartini Location: 46-442 Zoom: https://uqz. zoom. us/j/89362232168"
    }, {
    "id": 71,
    "url": "/libAUC/",
    "title": "LibAUC: A deep learning library for X-risk Optimization",
    "body": "2021/10/26 - In this talk, I will present our recent research efforts of developing a deep learning library called LibAUC, which is applicable for solving a variety of compositional measures.  I will first present some background about optimization for machine learning and then introduce you a broad family of compositional measures/objectives called X-risk. Then I will focus on optimizing two measures, AUROC and AUPRC by talking about our theoretical research for designing algorithms and improving complexities, and our applied research for winning medical AI challenges. Short Bio: Tianbao Yang is an Associate Professor of Computer Science and Engineering at Texas A&amp;M University. His research interests center round optimization, machine learning and AI. He received the best student paper award at COLT in 2012, NSF Career Award in 2019, and was named Dean’s Excellence in Research Scholar.  He is one of the early pioneers in Federated learning by publishing the first paper that inspires the Federated learning paradigm.  His group recently developed a deep learning library LibAUC, which has been downloaded more than 16K times and has won several competitions.  He has published more than 140 papers and served as associate editor of Neurocomputing, and area chairs of ICML/NeurIPS/AAAI/IJCAI.  More Details:  When: Wed 26 Oct 2022, at 1:00 pm (GMT+10) Speaker: A/Prof Tianbao Yang (Texas A&amp;M) Host: Dr Miao Xu Zoom: https://uqz. zoom. us/j/89362232168Video Recap: Zoom Recording "
    }, {
    "id": 72,
    "url": "/advanced-machine-perception/",
    "title": "Advancing Machine Perception for Artificial Intelligence Systems",
    "body": "2021/10/05 - Artificial intelligence (AI) techniques have impacted on our lives profoundly, such as providing more secured and resilient social environments and assisting more accurate medical diagnosis. As machine perception is one of the most fundamental and important components in computer vision and machine learning pipelines, achieving accurate and robust machine perception abilities in various situations plays a critical role in modern AI systems. In this talk, I will present our recent research results on machine perception algorithms in surveillance systems as well as in AI assisted medical diagnosis. In particular, I will introduce our face hallucination methods that significantly improve the perception ability of surveillance systems, thus ensuring the security of our society or facilitating forensics. I will also present our developed advanced brain tumor diagnosis algorithms to address the problems of limited medical data or missing medical data which often happen in real-world clinical practice, while improving the perception ability in AI assisted medical diagnosis systems. Short Bio: Xin Yu is a Senior Lecturer at the University of Technology Sydney. Previously, he was a research fellow at the Australian National University (ANU). He received a PhD degree from Tsinghua University and a second PhD degree from the Australian National University. His research interests cover a wide range of topics in Computer Vision and Machine Learning. He has published more than 70 papers on top-tier conference papers and journals, such as CVPR, ECCV, NeurIPS, ICLR, TPAMI, and IJCV. Dr. Yu was awarded ARC Discovery Early Career Researcher Award (DECRA) in 2022. He received Outstanding Reviewer Awards multiple times in the leading computer vision conferences. He also received Best Paper Honorable Mention Award in WACV 2020, and his paper was nominated for the Best Paper Award in CVPR 2020. He is a recipient of Google Research Scholar Award in 2021 (one of the five recipients in machine perception world-wide). He also won several Challenges in the workshops of CVPR, ECCV, ACCV, etc.  More Details:  When: Wed 5 Oct 2022, at 1:00 pm (GMT+10) Speaker: Dr Xin Yu Host: Prof Helen Huang Zoom: https://uqz. zoom. us/j/89362232168"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
    
 <!--endif page url is / -->
    


<!-- Now the rest of the posts with the usual loop but with an offset:4 on the first page so we can skeep the first 4 posts displayed above -->
    
<div class="row mt-3">
   
    <div class="col-md-8 main-loop">
        
        <h4 class="font-weight-bold spanborder"><span>All Talks</span></h4>
        

        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/nlp-Julien/">Leveraging semantics for recommendation at scale</a>
	</h2>
	<p class="excerpt">
	   In this talk, we present some of our recent work conducted at Amazon International Machine Learning Australia. First, we present a simple approach to address cold-start recommendation...
	</p>
	<small class="d-block text-muted">
		From <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#amazon">Amazon</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Mar 26, 2025
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/nlp-Julien/">
	<img class="w-100" src="/assets/images/speakers/julien.jpg" alt="Leveraging semantics for recommendation at scale">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/nlp-jiuxianggu/">Towards Multimodal Intelligence: Bridging Vision, Language, and Large-Scale Models</a>
	</h2>
	<p class="excerpt">
	   Multimodal intelligence is revolutionizing document understanding by enabling AI to process and reason across vision and language. This talk explores how large-scale models integrate ...
	</p>
	<small class="d-block text-muted">
		From <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#adobe">Adobe</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Mar 07, 2025
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/nlp-jiuxianggu/">
	<img class="w-100" src="/assets/images/speakers/jiuxianggu.jpg" alt="Towards Multimodal Intelligence: Bridging Vision, Language, and Large-Scale Models">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/nlp-yiweiwang/">From Next Token Prediction to Compliant AI Assistants: A Systematic Path toward Trustworthy Large Language Models</a>
	</h2>
	<p class="excerpt">
	   Language models are systems that can predict upcoming words” - this classical definition of NLP models forms the basis of LLMs becoming responsive text completion models. However, suc...
	</p>
	<small class="d-block text-muted">
		From <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#uc merced">UC Merced</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Feb 28, 2025
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/nlp-yiweiwang/">
	<img class="w-100" src="/assets/images/speakers/yiweiwang.jpg" alt="From Next Token Prediction to Compliant AI Assistants: A Systematic Path toward Trustworthy Large Language Models">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/showo-mikeshou/">No.25-01 Show-o: One Single Transformer to Unify Multimodal Understanding and Generation</a>
	</h2>
	<p class="excerpt">
	   Exciting models have been developed in multimodal video understanding and generation, such as video LLM and video diffusion model. One emerging pathway to the ultimate intelligence is...
	</p>
	<small class="d-block text-muted">
		From <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#nus">NUS</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Feb 27, 2025
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/showo-mikeshou/">
	<img class="w-100" src="/assets/images/speakers/mikes.jpg" alt="No.25-01 Show-o: One Single Transformer to Unify Multimodal Understanding and Generation">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/synt-yunzhi/">No.24-20 Controllable Visual Synthesis via Structural Representation</a>
	</h2>
	<p class="excerpt">
	   End-to-end neural approaches have revolutionized visual generation, producing stunning outputs from natural language prompts. However, precise controls remain challenging through dire...
	</p>
	<small class="d-block text-muted">
		From <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#stanford">Stanford</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Dec 13, 2024
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/synt-yunzhi/">
	<img class="w-100" src="/assets/images/speakers/yunzhi.jpg" alt="No.24-20 Controllable Visual Synthesis via Structural Representation">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/llmrec-jianling/">No.24-19 When LLMs Meet Recommendations: Scalable Hybrid Approaches to Enhance User Experiences</a>
	</h2>
	<p class="excerpt">
	   While LLMs offer powerful reasoning and generalization capabilities for user understanding and long-term planning in recommendation systems, their latency and cost hinder direct appli...
	</p>
	<small class="d-block text-muted">
		From <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#deepmind">Deepmind</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Dec 09, 2024
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/llmrec-jianling/">
	<img class="w-100" src="/assets/images/speakers/jianling.jpg" alt="No.24-19 When LLMs Meet Recommendations: Scalable Hybrid Approaches to Enhance User Experiences">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/ds-lclm-tianyu/">No.24-18 Developing Effective Long-Context Language Models</a>
	</h2>
	<p class="excerpt">
	   In this talk, I will share our journey behind developing an effective long-context language model. I’ll begin by introducing our initial approach of using parallel context encoding (C...
	</p>
	<small class="d-block text-muted">
		From <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#princeton">Princeton</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Dec 04, 2024
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/ds-lclm-tianyu/">
	<img class="w-100" src="/assets/images/speakers/tianyu.JPEG" alt="No.24-18 Developing Effective Long-Context Language Models">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/ds-vlm-feng/">No.24-17 Mitigating Distribution Shifts in Using Pre-trained Vision-Language Models</a>
	</h2>
	<p class="excerpt">
	   Benefiting from large-scale image-text pair datasets, powerful pre-trained vision-language models (VLMs, such as CLIP) enable many real-world applications, e.g., zero-shot classificat...
	</p>
	<small class="d-block text-muted">
		From <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#unimelb">UniMelb</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Dec 02, 2024
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/ds-vlm-feng/">
	<img class="w-100" src="/assets/images/speakers/feng_liu.png" alt="No.24-17 Mitigating Distribution Shifts in Using Pre-trained Vision-Language Models">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/gnn-wild-qitian/">No.24-16 Towards Graph Machine Learning in the Wild</a>
	</h2>
	<p class="excerpt">
	   Learning on graphs is a long-standing and fundamental challenge in machine learning and recent works have demonstrated solid progress in this area. However, most existing models tacit...
	</p>
	<small class="d-block text-muted">
		From <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#mit">MIT</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Nov 27, 2024
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/gnn-wild-qitian/">
	<img class="w-100" src="/assets/images/speakers/qitian.png" alt="No.24-16 Towards Graph Machine Learning in the Wild">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/eval-wenting/">No.24-15 Evaluation and Reasoning in Real-world Scenarios</a>
	</h2>
	<p class="excerpt">
	   User queries in natural settings, such as “provide a design for a disk topology for a NAS built on TrueNAS Scale, as well as a dataset layout,” differ significantly from those produce...
	</p>
	<small class="d-block text-muted">
		From <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#cornell university">Cornell University</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Nov 20, 2024
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/eval-wenting/">
	<img class="w-100" src="/assets/images/speakers/wenting.png" alt="No.24-15 Evaluation and Reasoning in Real-world Scenarios">
	</a>
	</div>

</div>
        
        
        
        <div class="mt-5">
         <!-- Pagination links -->
            
            <ul class="pagination"> 
              
                <li class="page-item"><a class="page-link" href="/">&laquo; Prev</a></li>
              

              
                
                <li class="page-item"><a class="page-link" href="/">1</a></li>
                
              
                
                <li class="page-item disabled"><span class="webjeda page-link">2</span></li>
                
              
                
                <li class="page-item"><a class="page-link" href="/page3">3</a></li>
                
              
                
                <li class="page-item"><a class="page-link" href="/page4">4</a></li>
                
              
                
                <li class="page-item"><a class="page-link" href="/page5">5</a></li>
                
              
                
                <li class="page-item"><a class="page-link" href="/page6">6</a></li>
                
              

              
                <li class="page-item"><a class="page-link" href="/page3">Next &raquo;</a></li>
              
            </ul>
                  
        </div>
        
    </div>
    
    <div class="col-md-4">
        <div class="sticky-top sticky-top-offset">
    <h4 class="font-weight-bold spanborder"><span>Upcoming</span></h4>  
    <ol class="list-featured">				
           
    </ol>
</div>     
    </div>
    
</div>



</div>
    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>


    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>UQ-DS</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>
                <span><img src="/assets/images/uq.jpg" width="200"></span>
                <!--  Github Repo Star Btn-->
                <!-- <a class="text-dark ml-1" target="_blank" href="https://github.com/wowthemesnet/mundana-theme-jekyll"><i class="fab fa-github"></i> Fork on Github</a> -->

            </div>
            <div>
                Made with <a target="_blank" class="text-dark font-weight-bold" href="https://itee.uq.edu.au/data-science"> Data Science Discipline </a>.
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
