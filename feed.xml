<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2025-09-16T12:08:37+10:00</updated><id>/feed.xml</id><title type="html">UQ-DS-Seminar</title><subtitle>Official Site for Data Science Seminar Series, The University of Queensland.</subtitle><entry><title type="html">Beyond Visual Geometry- Toward Physical 3D Reconstruction</title><link href="/chuanxiazheng/" rel="alternate" type="text/html" title="Beyond Visual Geometry- Toward Physical 3D Reconstruction" /><published>2025-09-16T00:00:00+10:00</published><updated>2025-09-16T00:00:00+10:00</updated><id>/chuanxiazheng</id><content type="html" xml:base="/chuanxiazheng/"><![CDATA[<p>Recent progress in 3D Visual Geometry has led to impressive reconstruction and generation results using purely feed-forward deep networks. However, much of this progress remains limited to geometric surface modeling or photorealistic novel view synthesis, often overlooking crucial physical attributes such as occluded geometry, physical stability, and dynamic motion. In this talk, I will present a series of work that go beyond visible geometry. I will introduce Amodal3R, which defines amodal 3D reconstruction from occluded 2D images, predicting complete 3D geometry and appearance when significant portions are hidden from views. Next, I will discuss DSO, which aligns 3D generative models with physics based simulation feedback, ensuring reconstruction are not just visually plausible but physically sound under gravity. To this end, I will introduce Geo4D, to explore how one can build high-quality 4D reconstruction networks starting form video generators pre-trained on millions of videos.</p>

<p>[1] Wu, T., Zheng, C., Guan, F., Vedaldi, A., &amp; Cham, T. J, “Amodal3r: Amodal 3d reconstruction from occluded 2d images”. ICCV 2025.</p>

<p>[2] Li, R., Zheng, C., Rupprecht, C., &amp; Vedaldi, A. “DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness”. ICCV 2025.</p>

<p>[3] Jiang, Z., Zheng, C., Laina, I., Larlus, D., &amp; Vedaldi, A. Geo4d: Leveraging video generators for geometric 4d scene reconstruction. ICCV 2025.</p>

<h2 id="speaker-bio">Speaker Bio</h2>

<p>Chuanxia Zheng is a Nanyang Assistant Professor at Nanyang Technological University (NTU), where he leads the physical vision group. Before joining NTU, he is a MSCA fellow in Visual Geometry Group (VGG) at the University of Oxford, working with Andrea Vedaldi on feed-forward photorealistic 3D and 4D reconstruction. He is also a DAAD Ainet Fellow of Germany, a National Research Foundation (NRF) fellow of Singapore. He received his Ph.D. degree from NTU.</p>

<p>His research focuses on Creative AI, with the goal of developing systems that can perceive, reconstruct and interact with the physical world. The broader goal is to create realistic digital twins of the physical natural world, capturing diverse physical properties, such as geometry, appearance, occlusion, motion, interaction, gravity, sound and more.</p>

<h2 id="more-details">More Details</h2>

<ul>
  <li>When: Sep 26 (Friday) 2025, at 1-2pm (Brisbane time)</li>
  <li>Speaker: Chuanxia Zheng (Nanyang Assistant Professor at NTU)</li>
  <li>Host: Yujun Cai</li>
  <li>Zoom: <a href="https://uqz.zoom.us/j/89476202411">https://uqz.zoom.us/j/89476202411</a></li>
</ul>]]></content><author><name>YJ</name></author><category term="NTU" /><category term="3D" /><summary type="html"><![CDATA[Recent progress in 3D Visual Geometry has led to impressive reconstruction and generation results using purely feed-forward deep networks. However, much of this progress remains limited to geometric surface modeling or photorealistic novel view synthesis, often overlooking crucial physical attributes such as occluded geometry, physical stability, and dynamic motion. In this talk, I will present a series of work that go beyond visible geometry. I will introduce Amodal3R, which defines amodal 3D reconstruction from occluded 2D images, predicting complete 3D geometry and appearance when significant portions are hidden from views. Next, I will discuss DSO, which aligns 3D generative models with physics based simulation feedback, ensuring reconstruction are not just visually plausible but physically sound under gravity. To this end, I will introduce Geo4D, to explore how one can build high-quality 4D reconstruction networks starting form video generators pre-trained on millions of videos.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/speakers/chuanxia_zheng.png" /><media:content medium="image" url="/assets/images/speakers/chuanxia_zheng.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AI-Driven Solutions in Individualised Medicine- From Multimodal Omics Data to Disease Diagnosis and Biomarker Discovery</title><link href="/hamid/" rel="alternate" type="text/html" title="AI-Driven Solutions in Individualised Medicine- From Multimodal Omics Data to Disease Diagnosis and Biomarker Discovery" /><published>2025-07-12T00:00:00+10:00</published><updated>2025-07-12T00:00:00+10:00</updated><id>/hamid</id><content type="html" xml:base="/hamid/"><![CDATA[<p>Despite significant advancements in medical science and an increasing emphasis on precision medicine, the majority of medical diagnoses are still made after patients exhibit noticeable symptoms. Early diagnosis, detection, and phenotyping of diseases could afford patients and their caregivers the opportunity for timely interventions, potentially leading to improved disease management, better prognoses, and more efficient utilisation of healthcare resources. The rising adoption of personalised medicine by global health systems, coupled with recent advancements in Artificial Intelligence (AI) and Machine Learning (ML) technologies, presents a significant opportunity to leverage AI in personalised medicine. This approach aims to lower the barriers to the clinical integration of personalised medicine, addressing a critical unmet need in the healthcare sector. Dr. Rokny is a multidisciplinary scientist dedicated to employing state-of-the-art AI and ML methodologies in conjunction with multimodal omics data to unravel the mechanisms underlying genetic diseases and cancers. In his presentation, Dr. Rokny will first outline the scope of work undertaken by his Biomedical Machine Learning Laboratory (BML), which spans the application of ML and AI in genomics and spatial transcriptomics to the more recent deep graph representation learning models. He will delve into a specific projects, “TENNIS” and “SemanticST”, offering detailed insights. Additionally, he will discuss the challenges, limitations, and potential biases inherent in the deployment of AI in individualised medicine, refining its role in transforming healthcare.</p>

<h2 id="speaker-bio">Speaker Bio</h2>

<p>Dr. Rokny is a Scientia Senior Lecturer (e.q., Associate Professor) at UNSW Sydney and Adj Associate Professor at Concordia University, CANADA, obtaining his Ph.D in 2020 at UNSW Sydney. Dr. Rokny is also the Founder and Director of Australian Society of BioMedical Machine Learning with more than 4000 members. He is also the health data analytics program leader at the Macquarie Centre for Applied AI. During his career, he developed a deep cross-disciplinary expertise in Artificial Intelligence (AI), Bio Machine Learning, and Genomics. Dr. Rokny joined UNSW on a highly prestigious and competitive UNSW Scientia Program Fellowship in October 2019 and currently is the Director of UNSW BioMedical Machine Learning laboratory (BML), where he is supervising a group consisting of 20 researchers including HDR and post-doc supervision, further demonstrating his comprehensive leadership and project management capabilities. Dr. Rokny has held continuous peer reviewed salary and grant support for his entire career, including but not limited to two WA Department of Health Merit Fellowships, two UNSW Scientia Fellowship, Tyree Foundation Institute of Health Engineering, and ARC DECRA. Totally, throughout his career, he has received 28 prizes/awards and also was able to secure over $5.2M peer-reviewed grants as Chief Investigator (CI) including $3.75M as either sole CI or leading CI. Additionally, Dr. Rokny has produced over 80 peer-reviewed publications in AI method development and their applications in genomics, half of which are in the top 10% of the fields (45 as the leading author), evidencing his impact with over 4,100 citations and a h-index of 36. He has also a strong relationship with industry, in which his expertise in AI and ML has been funded &gt;$10M by his industrial collaborators. Dr. Rokny has also a demonstrated experience in leadership through his contribution to the University and school level committees. He has been invited to serve as keynote speaker and program committee member of national and international conferences (including HUGO 2020) and was also an editorial board member of Communications Biology and Neurocomputing. In addition to his research activities, Dr. Rokny is the course director of AI in Biology, Biomedical Informatics and Digital Medicine at UNSW Sydney.</p>

<h2 id="more-details">More Details</h2>

<ul>
  <li>When: July 18 2025, at 1-2pm (Brisbane time)</li>
  <li>Speaker: Hamid Alinejad Rokny (Scientia Senior Lecturer in UNSW)</li>
  <li>Host: Yujun Cai</li>
  <li>Venue: 78-411</li>
  <li>Zoom: <a href="https://uqz.zoom.us/j/83638226838">https://uqz.zoom.us/j/83638226838</a></li>
</ul>]]></content><author><name>YJ</name></author><category term="UNSW" /><category term="LLM" /><summary type="html"><![CDATA[Despite significant advancements in medical science and an increasing emphasis on precision medicine, the majority of medical diagnoses are still made after patients exhibit noticeable symptoms. Early diagnosis, detection, and phenotyping of diseases could afford patients and their caregivers the opportunity for timely interventions, potentially leading to improved disease management, better prognoses, and more efficient utilisation of healthcare resources. The rising adoption of personalised medicine by global health systems, coupled with recent advancements in Artificial Intelligence (AI) and Machine Learning (ML) technologies, presents a significant opportunity to leverage AI in personalised medicine. This approach aims to lower the barriers to the clinical integration of personalised medicine, addressing a critical unmet need in the healthcare sector. Dr. Rokny is a multidisciplinary scientist dedicated to employing state-of-the-art AI and ML methodologies in conjunction with multimodal omics data to unravel the mechanisms underlying genetic diseases and cancers. In his presentation, Dr. Rokny will first outline the scope of work undertaken by his Biomedical Machine Learning Laboratory (BML), which spans the application of ML and AI in genomics and spatial transcriptomics to the more recent deep graph representation learning models. He will delve into a specific projects, “TENNIS” and “SemanticST”, offering detailed insights. Additionally, he will discuss the challenges, limitations, and potential biases inherent in the deployment of AI in individualised medicine, refining its role in transforming healthcare.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/speakers/hamid.jpg" /><media:content medium="image" url="/assets/images/speakers/hamid.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Teach AI What It Doesn’t Know</title><link href="/xuefeng_du/" rel="alternate" type="text/html" title="Teach AI What It Doesn’t Know" /><published>2025-06-17T00:00:00+10:00</published><updated>2025-06-17T00:00:00+10:00</updated><id>/xuefeng_du</id><content type="html" xml:base="/xuefeng_du/"><![CDATA[<p>The remarkable capabilities of machine learning (ML) models, especially foundation models like GPT, have transformed numerous domains. However, these systems often falter in real-world settings, where they encounter unknown or out-of-distribution (OOD) inputs, and generate overconfident predictions or unreliable outputs. Ensuring their reliability is not only a technical challenge but also a fundamental requirement for their safe deployment.</p>

<p>In this talk, I will discuss my research on teaching ML models what they don’t know by developing foundational frameworks for reliable decision-making in the open world. This involves three core aspects: (1) designing novel algorithms for unknown-aware learning through adaptive outlier synthesis, enabling models to handle unfamiliar inputs without explicit knowledge of unknowns; (2) leveraging unlabeled data in the wild to detect and generalize across diverse real-world reliability challenges; and (3) addressing reliability blind spots in foundation models, such as hallucinations, malicious prompts, and noisy alignment data, through innovative mitigation strategies.</p>

<p>Through fundamental algorithmic development, theoretical insights, and practical applications, my research contributes to the responsible deployment of AI technologies. The talk will conclude with a forward-looking perspective on interdisciplinary collaborations and the roadmap for achieving robust, reliable AI systems that adapt to an ever-changing world.</p>

<h2 id="speaker-bio">Speaker Bio</h2>

<p>Sean (Xuefeng) Du is an incoming Assistant Professor at College of Computing and Data Science (CCDS), Nanyang Technological University, Singapore. He obtained his Ph.D. in Computer Sciences at UW-Madison advised by Prof. Sharon Li. His research interest is in reliable machine learning and the applications to foundation models and AI safety. His first-author papers have been recognized with multiple oral and spotlight presentations at NeurIPS and CVPR. He is a recipient of the Jane Street Graduate Research Fellowship, and Rising Stars in Data Science award.</p>

<h2 id="more-details">More Details</h2>

<ul>
  <li>When: June 26 2025, at 1-2pm (Brisbane time)</li>
  <li>Speaker: Sean (Xuefeng) Du (incoming Assistant Professor in NTU)</li>
  <li>Host: Yujun Cai</li>
  <li>Zoom: <a href="https://uqz.zoom.us/j/89394692123">https://uqz.zoom.us/j/89394692123</a></li>
</ul>]]></content><author><name>YJ</name></author><category term="NTU" /><category term="ML" /><summary type="html"><![CDATA[The remarkable capabilities of machine learning (ML) models, especially foundation models like GPT, have transformed numerous domains. However, these systems often falter in real-world settings, where they encounter unknown or out-of-distribution (OOD) inputs, and generate overconfident predictions or unreliable outputs. Ensuring their reliability is not only a technical challenge but also a fundamental requirement for their safe deployment.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/speakers/xuefeng_du.png" /><media:content medium="image" url="/assets/images/speakers/xuefeng_du.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Reasoning with Language Models</title><link href="/llm-nick/" rel="alternate" type="text/html" title="Reasoning with Language Models" /><published>2025-05-25T00:00:00+10:00</published><updated>2025-05-25T00:00:00+10:00</updated><id>/llm-nick</id><content type="html" xml:base="/llm-nick/"><![CDATA[<p>Language models are primarily trained via imitation on massive amounts of human data; as a result, they’re capable of performing a wide range of tasks but often lack the deep reasoning capabilities of classic AI systems like Deep Blue and AlphaGo. In this talk, I’ll first present core technical challenges related to “reasoning with language,” using my work on computer crossword solvers as a running example. Then, I’ll show how methods for “interactive reasoning” can enable human-AI teams to solve complex problems jointly. Finally, I’ll discuss my work on “explainable reasoning,” where the goal is to explain the decisions made by expert AI systems like AlphaGo in human-interpretable terms. I will conclude by sharing my views on the future of language model reasoning, agents, and interactive systems.</p>

<h2 id="speaker-bio">Speaker Bio</h2>

<p>Nicholas Tomlin is a final-year PhD student in the Berkeley NLP Group, where he is advised by Dan Klein. His work focuses primarily on reasoning and multi-agent interaction with language models. He has co-created systems such as The Berkeley Crossword Solver, the first superhuman computer crossword solver, as well as Ghostbuster, a state-of-the-art method for LLM detection. His work has been supported by grants from the NSF and FAR AI and has received media coverage from outlets such as Discover, Wired, and the BBC. He is also an incoming CDS Faculty Fellow at NYU and incoming assistant professor at TTIC.</p>

<h2 id="more-details">More Details</h2>

<ul>
  <li>When: May 29 2025, at 1-2pm (Brisbane time)</li>
  <li>Speaker: Nicholas Tomlin (UC Berkeley)</li>
  <li>Host: Dr Ruihong Qiu</li>
  <li>Zoom: <a href="https://uqz.zoom.us/j/86460266783">https://uqz.zoom.us/j/86460266783</a></li>
</ul>]]></content><author><name>YJ</name></author><category term="UC Berkeley" /><category term="llm" /><summary type="html"><![CDATA[Language models are primarily trained via imitation on massive amounts of human data; as a result, they’re capable of performing a wide range of tasks but often lack the deep reasoning capabilities of classic AI systems like Deep Blue and AlphaGo. In this talk, I’ll first present core technical challenges related to “reasoning with language,” using my work on computer crossword solvers as a running example. Then, I’ll show how methods for “interactive reasoning” can enable human-AI teams to solve complex problems jointly. Finally, I’ll discuss my work on “explainable reasoning,” where the goal is to explain the decisions made by expert AI systems like AlphaGo in human-interpretable terms. I will conclude by sharing my views on the future of language model reasoning, agents, and interactive systems.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/speakers/nick.png" /><media:content medium="image" url="/assets/images/speakers/nick.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Towards Efficient Novel View Synthesis</title><link href="/cv-qianyiwu/" rel="alternate" type="text/html" title="Towards Efficient Novel View Synthesis" /><published>2025-05-17T00:00:00+10:00</published><updated>2025-05-17T00:00:00+10:00</updated><id>/cv-qianyiwu</id><content type="html" xml:base="/cv-qianyiwu/"><![CDATA[<p>Novel view synthesis (NVS) from 2D images aims to generate unseen views of a scene given multiple input observations. It is a fundamental task in computer vision that has garnered significant attention due to recent advances in 3D representations and neural rendering. Techniques such as Neural Radiance Fields and 3D Gaussian Splatting have substantially improved NVS quality, yet the demand for more efficient approaches—in terms of space, time, and storage—remains a critical research direction. In this talk, I will present our recent efforts to address these challenges. Specifically, I will discuss two of our latest works: FCGS (ICLR 2025), which introduces a compression method for 3D Gaussian Splatting, and Pansplat (CVPR 2025), a feed-forward model designed for panoramic novel view synthesis.</p>

<h2 id="speaker-bio">Speaker Bio</h2>

<p>Qianyi Wu is a final-year PhD at Department of Data Science and AI, Monash University, under the supervision of Prof.Jianfei Cai. Qianyi received B.S. degree in Special Class for the Gifted Youth at University of Science and Technology of China (USTC) in 2016. He received M.Sc degree from Graphics and Geometric Computing Laboratory of the School of Mathematical Sciences at USTC in 2019, under the supervision of Prof. Juyong Zhang. He worked as a research scientist intern at Meta Reality Lab in 2024. His recent research area focuses on 3D reconstruction and generation.</p>

<h2 id="more-details">More Details</h2>

<ul>
  <li>When: May 23 2025, at 3-4pm (Brisbane time)</li>
  <li>Speaker: Qianyi Wu (Monash University)</li>
  <li>Host: Dr Yujun Cai</li>
  <li>Zoom: <a href="https://uqz.zoom.us/j/88094383147">https://uqz.zoom.us/j/88094383147</a></li>
</ul>]]></content><author><name>YJ</name></author><category term="Monash University" /><category term="cv" /><summary type="html"><![CDATA[Novel view synthesis (NVS) from 2D images aims to generate unseen views of a scene given multiple input observations. It is a fundamental task in computer vision that has garnered significant attention due to recent advances in 3D representations and neural rendering. Techniques such as Neural Radiance Fields and 3D Gaussian Splatting have substantially improved NVS quality, yet the demand for more efficient approaches—in terms of space, time, and storage—remains a critical research direction. In this talk, I will present our recent efforts to address these challenges. Specifically, I will discuss two of our latest works: FCGS (ICLR 2025), which introduces a compression method for 3D Gaussian Splatting, and Pansplat (CVPR 2025), a feed-forward model designed for panoramic novel view synthesis.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/speakers/qianyi.jpg" /><media:content medium="image" url="/assets/images/speakers/qianyi.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Towards Large Generative Recommendation Models: A Tokenization Perspective</title><link href="/ds-yupenghou/" rel="alternate" type="text/html" title="Towards Large Generative Recommendation Models: A Tokenization Perspective" /><published>2025-05-05T00:00:00+10:00</published><updated>2025-05-05T00:00:00+10:00</updated><id>/ds-yupenghou</id><content type="html" xml:base="/ds-yupenghou/"><![CDATA[<p>The emergence of large generative models is transforming the landscape of recommender systems, offering new opportunities through scaling laws and flexible content generation. One of the most fundamental components is action tokenization, which is the process of converting human-readable data (e.g., text or user-item interactions) into model-readable token sequences. This talk provides a tokenization-centric view of building effective and efficient generative recommendation models. We begin by introducing the backgrounds of semantic IDs and action tokenization techniques, as covered in our WWW’25 tutorial. We then introduce two recent work: aligning action tokens with LLMs (ICDE’24) and contextually tokenizing action sequences (ICML’25 spotlight).</p>

<h2 id="speaker-bio">Speaker Bio</h2>

<p>Yupeng Hou is a Ph.D. student at the University of California, San Diego, advised by Prof. Julian McAuley. He was a student researcher at Google DeepMind in 2024, working with Jianmo Ni, Derek Cheng, and Ed H. Chi. He previously received his M.E. and B.E. from Renmin University of China, advised by Prof. Wayne Xin Zhao. His work has been recognized as the Best Resource Paper Runner-up at CIKM 2022 and the Best Student Paper Runner-up at RecSys 2022. Yupeng is one of the leading developers of RecBole, a popular open-source recommendation library with over 3,700 GitHub stars. His current research focuses on generative recommendation and tokenization.</p>

<h2 id="more-details">More Details</h2>

<ul>
  <li>When: Thursday. 22 May 2025, at 1-2pm (Brisbane time)</li>
  <li>Speaker: Yupeng Hou (UCSD)</li>
  <li>Host: Dr Ruihong Qiu</li>
  <li>Zoom: <a href="https://uqz.zoom.us/j/86490391734">https://uqz.zoom.us/j/86490391734</a></li>
</ul>]]></content><author><name>YJ</name></author><category term="University of California" /><category term="San Diego (UCSD)" /><category term="Recommendation" /><summary type="html"><![CDATA[The emergence of large generative models is transforming the landscape of recommender systems, offering new opportunities through scaling laws and flexible content generation. One of the most fundamental components is action tokenization, which is the process of converting human-readable data (e.g., text or user-item interactions) into model-readable token sequences. This talk provides a tokenization-centric view of building effective and efficient generative recommendation models. We begin by introducing the backgrounds of semantic IDs and action tokenization techniques, as covered in our WWW’25 tutorial. We then introduce two recent work: aligning action tokens with LLMs (ICDE’24) and contextually tokenizing action sequences (ICML’25 spotlight).]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/speakers/yupenghou.jpg" /><media:content medium="image" url="/assets/images/speakers/yupenghou.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Would prompt work for graph learning? An exploration of few-shot learning on graphs</title><link href="/gnn-fangyuan/" rel="alternate" type="text/html" title="Would prompt work for graph learning? An exploration of few-shot learning on graphs" /><published>2025-04-11T00:00:00+10:00</published><updated>2025-04-11T00:00:00+10:00</updated><id>/gnn-fangyuan</id><content type="html" xml:base="/gnn-fangyuan/"><![CDATA[<p>Graph structures are prevalent across a variety of fields, including social networks, e-commerce, transportation, and biological systems. Within these graphs, numerous analytical and mining tasks can be identified, often aligning with link prediction, node classification, or graph classification. Graph Neural Networks (GNNs) have achieved significant success in these applications, primarily due to their capability to learn powerful graph representations. Nevertheless, the efficacy of GNNs often depends on the availability of labeled data, without which their performance may be compromised. This talk seeks to delve into learning paradigms that diverge from traditional supervised learning, in the context of few-shot learning on graphs. In particular, drawing inspiration from recent progress in language modeling, we pose an intriguing question: Can prompt-based learning be effectively adapted for graph data? Our talk will begin with a comprehensive overview of few-shot learning methodologies on graphs, followed by highlighting some of our representative works in this area.</p>

<h2 id="speaker-bio">Speaker Bio</h2>

<p>Dr. Yuan Fang is a tenure-track Assistant Professor at the School of Computing and Information Systems at Singapore Management University (SMU). He was previously a data scientist at DBS Bank and a research scientist at A*STAR. His research interests revolve around graph learning and its applications in recommender systems, social network analysis, and science for AI. Dr. Fang has published over 90 papers in leading conferences and journals, and his work has been featured as “Most Influential Paper in WWW’23” (PaperDigest, Sep 2024), as well as “Best Papers of VLDB13” (VLDB Journal Special Issue). He is a Young Associate Editor of Frontiers of Computer Science (Springer &amp; Higher Education Press), and serves various international conferences as organization committee member and area chair.</p>
<h2 id="more-details">More Details</h2>

<ul>
  <li>When: Thursday. 17 April 2025, at 1-2pm (Brisbane time)</li>
  <li>Speaker: Prof Yuan Fang (SMU)</li>
  <li>Host: Dr Yujun Cai</li>
  <li>Zoom: <a href="https://uqz.zoom.us/j/84511228842">https://uqz.zoom.us/j/84511228842</a></li>
</ul>]]></content><author><name>YJ</name></author><category term="Singapore Management University (SMU)" /><category term="LLM" /><summary type="html"><![CDATA[Graph structures are prevalent across a variety of fields, including social networks, e-commerce, transportation, and biological systems. Within these graphs, numerous analytical and mining tasks can be identified, often aligning with link prediction, node classification, or graph classification. Graph Neural Networks (GNNs) have achieved significant success in these applications, primarily due to their capability to learn powerful graph representations. Nevertheless, the efficacy of GNNs often depends on the availability of labeled data, without which their performance may be compromised. This talk seeks to delve into learning paradigms that diverge from traditional supervised learning, in the context of few-shot learning on graphs. In particular, drawing inspiration from recent progress in language modeling, we pose an intriguing question: Can prompt-based learning be effectively adapted for graph data? Our talk will begin with a comprehensive overview of few-shot learning methodologies on graphs, followed by highlighting some of our representative works in this area.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/speakers/fangyuan.jpg" /><media:content medium="image" url="/assets/images/speakers/fangyuan.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Towards Knowledgeable Foundation Models</title><link href="/knowllm-heng/" rel="alternate" type="text/html" title="Towards Knowledgeable Foundation Models" /><published>2025-04-07T00:00:00+10:00</published><updated>2025-04-07T00:00:00+10:00</updated><id>/knowllm-heng</id><content type="html" xml:base="/knowllm-heng/"><![CDATA[<p>Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance on knowledge reasoning tasks, owing to their implicit knowledge derived from extensive pretraining data. However, their inherent knowledge bases often suffer from disorganization and illusion, bias towards common entities, and rapid obsolescence. Consequently, LLMs frequently make up untruthful information, exhibit resistance to updating outdated knowledge, or struggle with generalizing across multiple languages. In this talk I will discuss several research directions that aim to make foundation models’ knowledge more accurate, organized, up-to-date and fair: (1) Where and How is Knowledge Stored in LLM? (2) How to Control LLM’s Knowledge? (3) How to Update LLM’s Dynamic Knowledge? (4) How to Bridge the Knowledge Gap between Natural Language and Unnatural Language?</p>

<h2 id="speaker-bio">Speaker Bio</h2>

<p>Heng Ji is a Tenured Full Professor and Associate Director for Research of Siebel School of Computing and Data Science, and a faculty member affiliated with Electrical and Computer Engineering Department, Coordinated Science Laboratory, and Carl R. Woese Institute for Genomic Biology of University of Illinois Urbana-Champaign. She is an Amazon Scholar. She is the Founding Director of Amazon-Illinois Center on AI for Interactive Conversational Experiences (AICE). She received Ph.D. in Computer Science from New York University. Her research interests focus on Natural Language Processing, especially on Multimedia Multilingual Information Extraction, Knowledge-enhanced Large Language Models and Vision-Language Models, and AI for Science. The awards she received include Outstanding Paper Award at ACL2024, two Outstanding Paper Awards at NAACL2024, “Young Scientist” by the World Laureates Association in 2023 and 2024, “Young Scientist” and a member of the Global Future Council on the Future of Computing by the World Economic Forum in 2016 and 2017, “Women Leaders of Conversational AI” (Class of 2023) by Project Voice, “AI’s 10 to Watch” Award by IEEE Intelligent Systems in 2013, NSF CAREER award in 2009, PACLIC2012 Best paper runner-up, “Best of ICDM2013” paper award, “Best of SDM2013” paper award, ACL2018 Best Demo paper nomination, ACL2020 Best Demo Paper Award, NAACL2021 Best Demo Paper Award, Google Research Award in 2009 and 2014, IBM Watson Faculty Award in 2012 and 2014 and Bosch Research Award in 2014-2018. She served as the associate editor for IEEE/ACM Transaction on Audio, Speech, and Language Processing, and the Program Committee Co-Chair of many conferences including NAACL-HLT2018 and AACL-IJCNLP2022. She was elected as the North American Chapter of the Association for Computational Linguistics (NAACL) secretary 2020-2023.</p>

<h2 id="more-details">More Details</h2>

<ul>
  <li>When: Thu. 10 April 2025, at 1-2pm (Brisbane time)</li>
  <li>Speaker: Prof Heng Ji (University of Illinois Urbana-Champaign)</li>
  <li>Host: Dr Ruihong Qiu</li>
  <li>Zoom: <a href="https://uqz.zoom.us/j/86395470812">https://uqz.zoom.us/j/86395470812</a> (No recording will be provided)</li>
</ul>]]></content><author><name>RQ</name></author><category term="University of Illinois Urbana-Champaign" /><category term="LLM" /><summary type="html"><![CDATA[Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance on knowledge reasoning tasks, owing to their implicit knowledge derived from extensive pretraining data. However, their inherent knowledge bases often suffer from disorganization and illusion, bias towards common entities, and rapid obsolescence. Consequently, LLMs frequently make up untruthful information, exhibit resistance to updating outdated knowledge, or struggle with generalizing across multiple languages. In this talk I will discuss several research directions that aim to make foundation models’ knowledge more accurate, organized, up-to-date and fair: (1) Where and How is Knowledge Stored in LLM? (2) How to Control LLM’s Knowledge? (3) How to Update LLM’s Dynamic Knowledge? (4) How to Bridge the Knowledge Gap between Natural Language and Unnatural Language?]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/speakers/HengJi.jpeg" /><media:content medium="image" url="/assets/images/speakers/HengJi.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AppAgent X—Making GUI Agents Smarter with Use</title><link href="/cv-chizhang/" rel="alternate" type="text/html" title="AppAgent X—Making GUI Agents Smarter with Use" /><published>2025-03-21T00:00:00+10:00</published><updated>2025-03-21T00:00:00+10:00</updated><id>/cv-chizhang</id><content type="html" xml:base="/cv-chizhang/"><![CDATA[<p>In recent years, the development of multimodal large language models has given rise to a new class of intelligent agents—GUI Agents—that can autonomously operate computers and smartphones through natural language, enabling automated office tasks and cross-application task execution. However, existing methods rely on step-by-step reasoning, resulting in high computational costs and low execution efficiency, especially for repetitive tasks.</p>

<p>To overcome this bottleneck, we introduce AppAgent X, an evolutionary GUI agent framework. Unlike traditional agents that repeatedly reason through each step, AppAgent X learns from its own operational experience, continuously generalizing and optimizing efficient behavior patterns. This enables the agent to become more efficient and intelligent with use.</p>

<p>This presentation will cover the core mechanisms of AppAgent X, experimental results, and its potential applications in the field of intelligent agents.</p>
<h2 id="speaker-bio">Speaker Bio</h2>

<p>Dr. Chi Zhang received his Ph.D. from the School of Computer Science and Engineering at Nanyang Technological University, Singapore. In 2024, he joined the School of Engineering at Westlake University as an Assistant Professor (PI, Ph.D. Supervisor) and founded the Artificial General Intelligence (AGI) Lab. Prior to this, he worked as a Research Scientist at Tencent from 2022 to 2024.</p>

<p>Dr. Zhang’s research focuses on multimodal models and generative artificial intelligence (GenAI). To date, he has published over 30 papers in top-tier AI conferences and journals, including CVPR, ICCV, NeurIPS, and TPAMI. He was named among the World’s Top 2% Scientists by Stanford University in both 2023 and 2024.</p>
<h2 id="more-details">More Details</h2>

<ul>
  <li>When: Wed. 2 April 2025, at 1-2pm (Brisbane time)</li>
  <li>Speaker: Prof Chi Zhang (Westlake University)</li>
  <li>Host: Dr Yujun Cai</li>
  <li>Zoom: <a href="https://uqz.zoom.us/j/88065580162">https://uqz.zoom.us/j/88065580162</a></li>
</ul>]]></content><author><name>YJ</name></author><category term="Westlake University" /><category term="LLM" /><summary type="html"><![CDATA[In recent years, the development of multimodal large language models has given rise to a new class of intelligent agents—GUI Agents—that can autonomously operate computers and smartphones through natural language, enabling automated office tasks and cross-application task execution. However, existing methods rely on step-by-step reasoning, resulting in high computational costs and low execution efficiency, especially for repetitive tasks.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/speakers/chizhang.jpg" /><media:content medium="image" url="/assets/images/speakers/chizhang.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Leveraging semantics for recommendation at scale</title><link href="/nlp-Julien/" rel="alternate" type="text/html" title="Leveraging semantics for recommendation at scale" /><published>2025-03-14T00:00:00+10:00</published><updated>2025-03-14T00:00:00+10:00</updated><id>/nlp-Julien</id><content type="html" xml:base="/nlp-Julien/"><![CDATA[<p>In this talk, we present some of our recent work conducted at Amazon International Machine Learning Australia. First, we present a simple approach to address cold-start recommendation by leveraging semantic information (ACM Recsys ’24). We then introduce a notion of generalization gap in collaborative filtering, and derive a geometric upper bound and a way to meaningfully utilize the geometry of the product metadata to improve recommendations (AISTATS ’25).  We finally present an application of sequential recommendation for complementary product selection (ACM WSDM ’25).</p>

<h2 id="speaker-bio">Speaker Bio</h2>

<p>Dr Julien Monteil is leading the Machine Learning group at Amazon International Machine Learning Australia, which primarily focuses on the Research and Development of recommender systems for Amazon customers globally. He is also an Adjunct Senior Lecturer at the University of Queensland, School of Civil Engineering. He has 12 years of post-PhD experience in the Research and Development of ML systems for customer-facing applications in retail, automotive, networking, healthcare. He developed and launched dozens of ML systems for Amazon, AWS, IBM, including many systems benefiting millions of customers monthly, and generating dozens of millions yearly in business impact. He authored 50+ peer-reviewed papers and 20+ patents in diverse research communities, including optimization and control, transportation, while now actively contributing to the Machine Learning and Recommender Systems communities.</p>

<h2 id="more-details">More Details</h2>

<ul>
  <li>When: Wed. 26 Mar 2025, at 11am.-12pm (Brisbane time)</li>
  <li>Speaker: Dr Julien Monteil, Amazon International Machine Learning (Australia)</li>
  <li>Host: Dr Rocky Chen</li>
  <li>Venue: 14-217</li>
  <li>Zoom: <a href="https://uqz.zoom.us/j/87431037194">https://uqz.zoom.us/j/87431037194</a></li>
</ul>]]></content><author><name>YJ</name></author><category term="Amazon" /><category term="Recommendation" /><summary type="html"><![CDATA[In this talk, we present some of our recent work conducted at Amazon International Machine Learning Australia. First, we present a simple approach to address cold-start recommendation by leveraging semantic information (ACM Recsys ’24). We then introduce a notion of generalization gap in collaborative filtering, and derive a geometric upper bound and a way to meaningfully utilize the geometry of the product metadata to improve recommendations (AISTATS ’25).  We finally present an application of sequential recommendation for complementary product selection (ACM WSDM ’25).]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/speakers/julien.jpg" /><media:content medium="image" url="/assets/images/speakers/julien.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>