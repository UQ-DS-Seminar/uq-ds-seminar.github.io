---
layout: post
title: "No.24-20 Controllable Visual Synthesis via Structural Representation"
author: RQ
categories: [Stanford]
image: assets/images/speakers/yunzhi.jpg
tags: [3D]
date: 2024-12-06
display-date: 2024-12-13
comments: True
---

End-to-end neural approaches have revolutionized visual generation, producing stunning outputs from natural language prompts. However, precise controls remain challenging through direct text-to-scene generation, as natural language alone lacks the precision needed for specifying complex visual relationships. This talk explores compositional visual representations bridging pre-trained language and visual generative models through programs for scene structure, words for semantic abstraction, and neural embeddings capturing visual identity. Our results show how such representations enable precise scene control while building on modern generative models' capabilities, suggesting a scalable path for controllable visual synthesis.

## Speaker Bio

Yunzhi Zhang is a PhD student at Stanford University, advised by Jiajun Wu. Her current research interest lies in learning structural representation and generative models from visual data. She is supported by the Stanford Interdisciplinary Graduate Fellowship.

## More Details

- When: Fri 13 Dec 2024, at 1 - 2 pm (Brisbane time)
- Speaker: Yunzhi Zhang (Stanford)
- Host: Ruihong Qiu
- Zoom: [https://uqz.zoom.us/j/83905648246](https://uqz.zoom.us/j/83905648246) [[Recording]](https://uqz.zoom.us/j/83905648246)
